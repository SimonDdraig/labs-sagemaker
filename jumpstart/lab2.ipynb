{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06920b65",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:DarkSeaGreen\">JumpStart Lab 2</span>\n",
    "\n",
    "This lab does the following:\n",
    "\n",
    "- Uses the endpoint created in Lab 1\n",
    "- Implements SageMaker application-autoscaling\n",
    "- Tests the functionality \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb29b8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Prepare Your Environment</span>\n",
    "### Note if you want a venv, see Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb8cd5",
   "metadata": {},
   "source": [
    "# Lab 2 Starts Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86ebb0",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Setup</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region\n",
    "# for the purpose of this lab, us-east-1, us-west-2, eu-west-1 has the broadest coverage of JumpStart models and instance types\n",
    "# if you provision in other regions, you may not have access to all the models or instance types, and may need to request increase of quotas for some instance types\n",
    "myRegion='us-east-1'\n",
    "\n",
    "# parameter store\n",
    "myParameterStoreEndpointName='doit-jumpstart-sagemaker-endpoint-name'\n",
    "myParameterStoreIAMARN='doit-jumpstart-sagemaker-iam-arn'\n",
    "\n",
    "# application auto scaling policy\n",
    "myEndpointScalingPolicyName='doit-jumpstart-sagemaker-endpoint-scaling-policy'\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "from certifi import where\n",
    "\n",
    "botoSession = boto3.Session(region_name=myRegion)\n",
    "\n",
    "# Configure boto3 to use certifi's certificates - helps avoid SSL errors if your system’s certificate store is out of date or missing root certs\n",
    "sts_client = boto3.client('sts', verify=where())\n",
    "myAccountNumber = sts_client.get_caller_identity()[\"Account\"]\n",
    "print(myAccountNumber)\n",
    "print(sts_client.get_caller_identity()[\"Arn\"])\n",
    "\n",
    "# create clients we can use later\n",
    "# iam\n",
    "iam = boto3.client('iam', region_name=myRegion, verify=where())\n",
    "# ssm\n",
    "ssm = boto3.client('ssm', region_name=myRegion, verify=where())\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tags added to all services we create\n",
    "myTags = [\n",
    "    {\"Key\": \"env\", \"Value\": \"non_prod\"},\n",
    "    {\"Key\": \"owner\", \"Value\": \"doit-jumpstart\"},\n",
    "    {\"Key\": \"project\", \"Value\": \"lab1\"},\n",
    "    {\"Key\": \"author\", \"Value\": \"simon\"},\n",
    "]\n",
    "myTagsDct = {\n",
    "    \"env\": \"non_prod\",\n",
    "    \"owner\": \"doit-jumpstart\",\n",
    "    \"project\": \"lab1\",\n",
    "    \"author\": \"simon\",\n",
    "}\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a8523",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">IAM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSageMakerExecutionRole():\n",
    "    \"\"\"\n",
    "    Gets a role required for SageMaker to run jobs on your behalf\n",
    "    Only needed if this is being run in a local IDE, not needed if in SageMaker Studio or SageMaker Notebook Instance\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        An IAM execution role ARN\n",
    "    \"\"\"\n",
    "\n",
    "    # get the role we created in the previous lab from the parameter store\n",
    "    response = ssm.get_parameter(Name=myParameterStoreIAMARN)\n",
    "    myRoleSageMakerExecutionARN = response['Parameter']['Value']\n",
    "    print(f\"Retrieved role from parameter store: {myRoleSageMakerExecutionARN}\")    \n",
    "\n",
    "    return myRoleSageMakerExecutionARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708857ff",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get Execution Role and Session</span>\n",
    "- SageMaker requires an execution role to assume on your behalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e08fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "try:\n",
    "    # if this is being run in a SageMaker AI JupyterLab Notebook\n",
    "    myRoleSageMakerExecutionARN = get_execution_role()\n",
    "except:\n",
    "    # if this is being run in a local IDE - we need to create our own role\n",
    "    myRoleSageMakerExecutionARN = getSageMakerExecutionRole()\n",
    "\n",
    "# make sure we get a session in the correct region (needed as it can use the aws configure region if running this locally\n",
    "sageMakerSession = Session(boto_session=botoSession)\n",
    "\n",
    "print(myRoleSageMakerExecutionARN)\n",
    "print(sageMakerSession)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3553bb10",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get the Endpoint from Lab 1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcada497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the endpoint created in lab1\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# get the endpoint name from parameter store\n",
    "response = ssm.get_parameter(\n",
    "    Name=myParameterStoreEndpointName\n",
    ")\n",
    "endpointName = response['Parameter']['Value']\n",
    "print(f\"Using endpoint name: {endpointName}\")  \n",
    "\n",
    "# create a predictor to interact with the endpoint - need to specify the default serializer and deserializer this time\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpointName,\n",
    "    sagemaker_session=sageMakerSession,\n",
    "    serializer=JSONSerializer(),      \n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the endpoint\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"body\": {\n",
    "            \"inputs\": \"Write a Python function to check if a number is prime\",\n",
    "            \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.2, \"top_p\": 0.9},\n",
    "        },\n",
    "        \"content_type\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "    },\n",
    "    {\n",
    "        \"body\": {\n",
    "            \"inputs\": \"Describe what a llm model can do for someone who is sceptical about them\",\n",
    "            \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.2, \"top_p\": 0.9},\n",
    "        },\n",
    "        \"content_type\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for payload in example_payloads:\n",
    "    body = payload.body if hasattr(payload, \"body\") else payload[\"body\"]\n",
    "    response = predictor.predict(body)\n",
    "    response = response[0] if isinstance(response, list) else response\n",
    "    print(\"Input:\\n\", body, end=\"\\n\\n\")\n",
    "    print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e923450",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Create Scalability Plan</span>\n",
    "- Uses SageMaker Application Auto Scaling\n",
    "- Works especially well for generative AI models, which are typically concurrency-bound and can take many seconds to complete each inference request\n",
    "- Using the new high-resolution metrics allow you to greatly decrease the time it takes to scale up an endpoint using Application Auto Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef977508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/huggingfacetgi/meta-llama/llama3-8b/faster-autoscaling/realtime-endpoints/FasterAutoscaling-SME-Llama3-8B-AppAutoScaling.ipynb\n",
    "# https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-launches-faster-auto-scaling-for-generative-ai-models/\n",
    "# https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html\n",
    "\n",
    "# define a new auto scaling target for Application Auto Scaling\n",
    "# we will use target tracking scaling - scale a resource based on a target value for a specific CloudWatch metric\n",
    "# auto scaling\n",
    "autoScaling = boto3.client('application-autoscaling', region_name=myRegion, verify=where())\n",
    "variantName = \"AllTraffic\"\n",
    "ResourceId  = \"endpoint/\" + endpointName + \"/variant/\" + variantName\n",
    "\n",
    "# Register scalable target\n",
    "scalableTarget = autoScaling.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=ResourceId,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=3,  # Replace with your desired maximum instances\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target tracking scaling policy\n",
    "# this is a target tracking policy that uses the new high-resolution metrics for SageMaker endpoints\n",
    "# you can also create a step-scaling policy if you prefer\n",
    "# a step-scaling policy is more complex to set up, but gives you more control over how your endpoint scales\n",
    "\n",
    "# create a policy that scales out when the endpoint receives more than n ConcurrentRequestsPerModel\n",
    "# this new metric will be tracked when th predefined metric type used below is SageMakerVariantConcurrentRequestsPerModelHighResolution\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/application-autoscaling/client/put_scaling_policy.html\n",
    "# NOTE this creates alarms with default thresholds, you can modify these alarms after creation if you want to change the thresholds\n",
    "# Eg 90 datapoints within 15 minutes\" is the default conservative behavior for scale-in to prevent flapping\n",
    "# you can change this to a more aggressive scale-in policy if you want to scale in faster by modifying the alarm created though not recommended\n",
    "# AWS automatically sets the scale-out and scale-in thresholds based on the TargetValue you specify in your target tracking policy\n",
    "# these thresholds determine when your endpoint variant scales up or down\n",
    "# sageMaker automatically populates the metric data for your endpoint, so no manual configuration is needed.\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-add-code-define.html#endpoint-auto-scaling-add-code-high-res\n",
    "targetTrackingPolicyResponse = autoScaling.put_scaling_policy(\n",
    "    PolicyName=myEndpointScalingPolicyName,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=ResourceId,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0,  # Scaling triggers when endpoint receives 5 ConcurrentRequestsPerModel\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantConcurrentRequestsPerModelHighResolution\"\n",
    "        },\n",
    "        \"ScaleInCooldown\": 180,  # Cooldown period after scale-in activity\n",
    "        \"ScaleOutCooldown\": 180,  # Cooldown period after scale-out activity\n",
    "    },\n",
    ")\n",
    "\n",
    "# print(target_tracking_policy_response)\n",
    "print(f\"[b]Policy ARN:[/b] [i blue]{targetTrackingPolicyResponse['PolicyARN']}\")\n",
    "\n",
    "# print Cloudwatch Alarms\n",
    "alarms = targetTrackingPolicyResponse[\"Alarms\"]\n",
    "\n",
    "for alarm in alarms:\n",
    "    print(f\"[b]Alarm Name:[/b] [b magenta]{alarm['AlarmName']}\")\n",
    "    # print(f\"[b]Alarm ARN:[/b] [i green]{alarm['AlarmARN']}[/i green]\")\n",
    "    print(\"===\" * 15)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece097a3",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Test Scalability Plan</span>\n",
    "- Lets just test the endpoint first, make sure its all good\n",
    "- Simulate load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5392bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets just test the endpoint again to make sure it still works\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"body\": {\n",
    "            \"inputs\": \"Please explain what load testing is and why its important in reference to sagemaker endpoints\",\n",
    "            \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.2, \"top_p\": 0.9},\n",
    "        },\n",
    "        \"content_type\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for payload in example_payloads:\n",
    "    body = payload.body if hasattr(payload, \"body\") else payload[\"body\"]\n",
    "    response = predictor.predict(body)\n",
    "    response = response[0] if isinstance(response, list) else response\n",
    "    print(\"Input:\\n\", body, end=\"\\n\\n\")\n",
    "    print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to use locust to simulate load on the endpoint\n",
    "# https://docs.locust.io/en/stable/ \n",
    "# https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/\n",
    "# see the locust_script_lab2.py file for details of the load test\n",
    "# it gathers the endpoint name, etc via os environment vars we export below\n",
    "# run this cell, then paste and run in a terminal window, make sure its run in your virtual environment created in lab 1, or in your own that has boto3 and locust installed\n",
    "\n",
    "print (\"export AWS_REGION={}\".format(myRegion))\n",
    "print (\"export ENDPOINT_NAME={}\".format(endpointName))\n",
    "print (\"export CONTENT_TYPE={}\".format(\"application/json\"))\n",
    "print (\"export PAYLOAD='{}'\".format('{\"inputs\": \"Please explain what load testing is and why its important in reference to sagemaker endpoints\"}'))\n",
    "print (\"export HOST={}\".format('http://localhost'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94932b41",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# these are picked up by the locust file\n",
    "# Paste the following in a terminal window, make sure its run in your virtual environment created in lab 1, or in your own that has boto3 and locust installed\n",
    "# LOCUST_USERS is the number of simulated users\n",
    "# LOCUST_SPAWN_RATE is the rate per second to spawn (add new) users - so 20 users at rate of 2 means add 2 users every second, so take 10 seconds to get to 20 users\n",
    "# LOCUST_RUN_TIME is how long to run the test for\n",
    "export LOCUST_USERS=20\n",
    "export LOCUST_SPAWN_RATE=2\n",
    "export LOCUST_RUN_TIME=10m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be23fb",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Before Locust is Run</span>\n",
    "- Go to the CloudWatch console\n",
    "- Monitor the alarm being target tracked for ConcurrentRequestsPerModel, eg \n",
    "  - TargetTracking-endpoint/*endpoint name*-Alarm**High**-*uuid*\n",
    "  - TargetTracking-endpoint/*endpoint name*-Alarm**Low**-*uuid*\n",
    "- Run the cell below to monitor the instance count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "endpoint_name = endpointName\n",
    "region = myRegion\n",
    "poll_interval = 10    # seconds between checks\n",
    "\n",
    "# --- Clients ---\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "cw_client = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "print(f\"Monitoring endpoint '{endpoint_name}' variants (press Ctrl+C to stop)...\\n\")\n",
    "print(\"Alarms for reference:\")\n",
    "print(f\"High Scaling In Alarm: {alarms[1]['AlarmName']} | Scale in cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when below {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n",
    "print(f\"Low Scaling In Alarm: {alarms[1]['AlarmName']} | Scale in cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when below {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # --- Describe endpoint variants ---\n",
    "        response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        for variant in response[\"ProductionVariants\"]:\n",
    "            variant_name = variant[\"VariantName\"]\n",
    "            current_instances = variant[\"CurrentInstanceCount\"]\n",
    "            desired_instances = variant[\"DesiredInstanceCount\"]\n",
    "\n",
    "            # --- Fetch latest ConcurrentRequestsPerModel metric ---\n",
    "            end_time = datetime.utcnow()\n",
    "            start_time = end_time - timedelta(seconds=poll_interval*2)  # small window to get the latest datapoint\n",
    "\n",
    "            # get the ConcurrentRequestsPerModel metric for this variant\n",
    "            metric_resp = cw_client.get_metric_statistics(\n",
    "                Namespace=\"AWS/SageMaker\",\n",
    "                MetricName=\"ConcurrentRequestsPerModel\",\n",
    "                Dimensions=[\n",
    "                    {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                    {\"Name\": \"VariantName\", \"Value\": variant_name},\n",
    "                ],\n",
    "                StartTime=start_time,\n",
    "                EndTime=end_time,\n",
    "                Period=poll_interval,\n",
    "                Statistics=[\"Average\"],\n",
    "            )\n",
    "\n",
    "            datapoints = metric_resp.get(\"Datapoints\", [])\n",
    "            concurrent_requests = round(datapoints[-1][\"Average\"], 2) if datapoints else 0\n",
    "\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Variant: {variant_name} | \"\n",
    "                f\"Current instances: {current_instances} | Desired instances: {desired_instances} | \"\n",
    "                f\"ConcurrentRequestsPerModel: {concurrent_requests} | \"\n",
    "                f\"High Scaling Out Alarm State: {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['StateValue']} | \"\n",
    "                f\"Low Scaling In Alarm State: {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['StateValue']}\"\n",
    "            )\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Monitoring stopped.\")\n",
    "    # print each alarm name and description on a new line\n",
    "    print(\"Alarms for reference:\")\n",
    "    print(f\"High Scaling In Alarm: {alarms[1]['AlarmName']} | Scale in cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when below {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n",
    "    print(f\"Low Scaling In Alarm: {alarms[1]['AlarmName']} | Scale in cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when below {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d9307",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Start Locust</span>\n",
    "- Paste the command into your terminal window in your venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa72d8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# now we're going to use locust to simulate load on the endpoint\n",
    "# https://docs.locust.io/en/stable/ \n",
    "# https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/\n",
    "# see the locust_script_lab2.py file for details of the load test\n",
    "# it gathers the endpoint name, etc from the parameter store where we stored it in lab 1\n",
    "\n",
    "# Paste the following in a terminal window, make sure its run in your virtual environment created in lab 1, or in your own that has boto3 and locust installed\n",
    "# this will run in headless mode, use the properties just exported, and write csv logs for use in graph cells below\n",
    "locust -f locust_script_lab2.py --headless -u $LOCUST_USERS -r $LOCUST_SPAWN_RATE --run-time $LOCUST_RUN_TIME --host http://localhost --csv results --csv-full-history\n",
    "\n",
    "# OR\n",
    "# if you want to see the locust web UI, then run without --headless and point your browser at http://localhost:8089\n",
    "# it will still write the csv files for use in graph cells below\n",
    "locust -f locust_script_lab2.py -u $LOCUST_USERS -r $LOCUST_SPAWN_RATE --run-time $LOCUST_RUN_TIME --host http://localhost --csv results --csv-full-history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e70e85",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Review Stats</span>\n",
    "- After Locust has finished, we can review the stats it wrote as csv files\n",
    "- NOTE the endpoint will not scale back in for 15 minutes (value may change depending on the scaling policy) after locust has finished its run - see above for a description of this\n",
    "  - therefore when viewing the charts below, it will not reflect a scale in event for 15 minutes \n",
    "\n",
    "#### SageMaker Endpoint Latency Under Load\n",
    "  - See how your SageMaker endpoint responds as load increases, and correlate that with autoscaling events\n",
    "  - NOTE this uses the results csv file written by locust, so wait until locust finishes before running this and subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88272da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot response times over time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the full history file\n",
    "df = pd.read_csv(\"results_stats_history.csv\")\n",
    "df.head()\n",
    "\n",
    "# Plot response times over time\n",
    "plt.plot(df[\"Timestamp\"], df[\"95%\"], label=\"p95 latency (ms)\")\n",
    "plt.plot(df[\"Timestamp\"], df[\"99%\"], label=\"p99 latency (ms)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.title(\"SageMaker Endpoint Latency Under Load\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22a6ff",
   "metadata": {},
   "source": [
    "#### Concurrent Users vs. Request Rate\n",
    "- Total RPS (Requests per second) → how busy your endpoint is at each interval\n",
    "- Concurrent Users → how many simulated users are active at that moment\n",
    "- Compare these to your endpoint instance count and scaling policy in SageMaker to see if the scaling is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3db877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Concurrent Users and Requests per Second\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Left Y-axis: Requests per second\n",
    "ax1.plot(df[\"Timestamp\"], df[\"Requests/s\"], color=\"tab:blue\", label=\"Total RPS\")\n",
    "ax1.set_xlabel(\"Time (s)\")\n",
    "ax1.set_ylabel(\"Requests per second\", color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# Right Y-axis: Number of concurrent users\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df[\"Timestamp\"], df[\"User Count\"], color=\"tab:orange\", label=\"Concurrent Users\")\n",
    "ax2.set_ylabel(\"Concurrent Users\", color=\"tab:orange\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n",
    "\n",
    "# Titles & Legends\n",
    "plt.title(\"Concurrent Users vs. Request Rate\")\n",
    "fig.tight_layout()\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b9b05",
   "metadata": {},
   "source": [
    "#### Concurrent Requests vs. Instance Count\n",
    "- Show when the instance scales\n",
    "- Pulls SageMaker endpoint metrics from CloudWatch\n",
    "- Handles empty or missing data gracefully\n",
    "- Converts them into a DataFrame for analysis\n",
    "- Plots the metric values over time\n",
    "- If the instance has not scaled in yet, its probably still within the 15 minutes (may be different depending on policy) of the scale in threshold, check the alarm in the console for details\n",
    "- NOTE there is a lookback in minutes defined below, so the cell output is only relevant if its run shortly after locust has finished, any longer and it may not get results from the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec17bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "lookback_minutes = 60  # how far back to fetch metrics\n",
    "\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "aas = boto3.client(\"application-autoscaling\", region_name=region)\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(minutes=lookback_minutes)\n",
    "\n",
    "# --- 1. Get CloudWatch metric: ConcurrentRequestsPerModel ---\n",
    "metrics = cw.get_metric_statistics(\n",
    "    Namespace=\"AWS/SageMaker\",\n",
    "    MetricName=\"ConcurrentRequestsPerModel\",\n",
    "    Dimensions=[{\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": variant_name}],\n",
    "    StartTime=start_time,\n",
    "    EndTime=end_time,\n",
    "    Period=60,\n",
    "    Statistics=[\"Average\"],\n",
    ")\n",
    "\n",
    "datapoints = metrics.get(\"Datapoints\", [])\n",
    "cw_df = pd.DataFrame(datapoints)\n",
    "\n",
    "if cw_df.empty:\n",
    "    print(\"⚠️ No CloudWatch datapoints found for ConcurrentRequestsPerModel\")\n",
    "else:\n",
    "    cw_df[\"Timestamp\"] = pd.to_datetime(cw_df[\"Timestamp\"])\n",
    "    cw_df.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "# --- 2. Get Current & Desired Instance Counts ---\n",
    "scalable_target = aas.describe_scalable_targets(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceIds=[f\"endpoint/{endpoint_name}/variant/{variant_name}\"],\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\"\n",
    ")\n",
    "desired_min = scalable_target[\"ScalableTargets\"][0][\"MinCapacity\"]\n",
    "desired_max = scalable_target[\"ScalableTargets\"][0][\"MaxCapacity\"]\n",
    "\n",
    "# --- 3. Get actual scaling activity history (DesiredInstanceCount changes) ---\n",
    "scaling_history = aas.describe_scaling_activities(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{endpoint_name}/variant/{variant_name}\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MaxResults=20\n",
    ")\n",
    "\n",
    "scaling_df = pd.DataFrame(scaling_history.get(\"ScalingActivities\", []))\n",
    "\n",
    "if not scaling_df.empty:\n",
    "    scaling_df[\"Timestamp\"] = pd.to_datetime(scaling_df[\"StartTime\"])\n",
    "    # Check if timestamps are timezone-aware\n",
    "    tzinfo = scaling_df[\"Timestamp\"].dt.tz  # will be None if naive\n",
    "    lookback_start = datetime.now(tz=tzinfo) - timedelta(minutes=lookback_minutes)\n",
    "\n",
    "    # Define cutoff\n",
    "    cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=lookback_minutes)\n",
    "    # Restrict to only rows newer than cutoff\n",
    "    scaling_df = scaling_df[scaling_df[\"Timestamp\"] >= cutoff_time].copy()\n",
    "    # extract the instance count\n",
    "    scaling_df[\"NewCapacity\"] = scaling_df[\"Description\"].str.extract(r\"(\\d+)(?=\\D*$)\").astype(float)\n",
    "    # Insert synthetic row at lookback start\n",
    "    synthetic_row = pd.DataFrame({\n",
    "        \"Timestamp\": [lookback_start],\n",
    "        \"NewCapacity\": [1.0],  # default instance count\n",
    "        \"StatusCode\": [\"Simulated\"],\n",
    "    })\n",
    "    scaling_df = pd.concat([synthetic_row, scaling_df], ignore_index=True)\n",
    "\n",
    "    # Sort for plotting\n",
    "    scaling_df.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "# --- 4. Plot everything together ---\n",
    "if not cw_df.empty:\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot concurrency (left axis)\n",
    "    ax1.plot(cw_df[\"Timestamp\"], cw_df[\"Average\"], marker=\"o\", color=\"tab:blue\", label=\"ConcurrentRequestsPerModel\")\n",
    "    ax1.set_xlabel(\"Time\")\n",
    "    ax1.set_ylabel(\"Concurrent Requests\", color=\"tab:blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    # Plot desired instance count (right axis)\n",
    "    ax2 = ax1.twinx()\n",
    "    if not scaling_df.empty:\n",
    "        ax2.step(scaling_df[\"Timestamp\"], scaling_df[\"NewCapacity\"], where=\"post\", color=\"tab:red\", label=\"DesiredInstanceCount\")\n",
    "    ax2.set_ylabel(\"Instance Count\", color=\"tab:red\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "    ax2.set_ylim(0, desired_max + 1)\n",
    "\n",
    "    fig.suptitle(f\"SageMaker Endpoint Scaling: {endpoint_name}/{variant_name}\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(cw_df.head(10))\n",
    "    if not scaling_df.empty:\n",
    "        display(scaling_df[[\"Timestamp\", \"NewCapacity\", \"StatusCode\", \"StatusMessage\"]].head(10))\n",
    "        display(scaling_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3d83b",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">SageMaker Inference Recommender</span>\n",
    "- Helps you select the best instance type and configuration for your ML models and workloads\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html\n",
    "- NOTE NEEDS TO BE INVESTIGATED TO SEE IF IT WORKS WITH LLM REAL TIME ENDPOINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfc496",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Clean Up Architecture</span>\n",
    "### <span style=\"color:Red\">Only do this if you have finished with this lab and any labs that depend on it!</span>\n",
    "##### It will delete all architecture created, make sure you no longer need any of it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when finished with the endpoint, delete it\n",
    "# endpoint and all other architecture is deleted in lab 1\n",
    "# nothing to do here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
