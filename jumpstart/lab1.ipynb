{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06920b65",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:DarkSeaGreen\">JumpStart Lab 1</span>\n",
    "\n",
    "This lab does the following:\n",
    "\n",
    "- Provision a model via Jumpstart\n",
    "- Create a JumpStart endpoint\n",
    "- Interacts with the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb29b8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Prepare Your Environment</span>\n",
    "### Requirements for this Jupyter Notebook Lab if running in VSCode or equivalent local IDE\n",
    "##### Note these are macOS specific\n",
    "- Credentials\n",
    "  - You need credentials to your AWS account to execute this Jupyter Lab if running locally from your laptop\n",
    "    - Locally: Credentials and therefore permissions asscociated with the IAM user (with CLI access enabled) are provided by AWS configure connection to your AWS account\n",
    "    - Cloud: Permissions provided via logged in user\n",
    "- Installers:\n",
    "  - Pip\n",
    "    - Python libraries\n",
    "    - Works inside Python envs\n",
    "  - homebrew (brew) (mac)\n",
    "    - System software, tools, and dependencies\n",
    "    - Works at OS level\n",
    "\n",
    "- Run the commands of the cell below in a terminal window to create a virtual environment if you need one\n",
    "  - Note check your Python version first, then if ok, copy the rest and run in terminal window\n",
    "  - Note if you copy and paste the multiple lines and run as one you will get zsh: command not found: # errors because of the comments, but you can ignore\n",
    "  - Remember to restart the kernel to pick up the new venv\n",
    "  - The venv can be deleted via the last cell in this notebook iof no longer needed\n",
    "- If you already have a virtual environment, then just activate it as shown in the second cell below\n",
    "  - Venv (can be created below) used by this notebook is *venv-jumpstart-lab1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e8873",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check your credentials (AWS identity) to confirm you are using the right credentials, can also run in a terminal window (remove the !)\n",
    "!aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27832229",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "### STOP ###\n",
    "### IF USING THIS NOTEBOOK IN A SAGEMAKER JUPYTER NOTEBOOK INSTANCE, THEN SKIP TO THE NEXT CELL ###\n",
    "### OTHERWISE, IF USING VSCODE OR EQUIVALENT LOCAL IDE, THEN CONTINUE BELOW ###\n",
    "### This script is for setting up your environment for the JumpStart Lab 1 ###\n",
    "# do you need to upgrade python first? Your available version of Python is used to create the virtual environment\n",
    "python3 --version\n",
    "\n",
    "### STOP ###\n",
    "### DO YOU NEED TO UPGRADE PYTHON ###\n",
    "# upgrade to the latest version of python if required\n",
    "brew install python\n",
    "# restart vscode to pickup new version of python\n",
    "python3 --version\n",
    "\n",
    "### STOP ###\n",
    "### OK IF YOU HAVE THE CORRECT VERSION OF PYTHON, CONTINUE ###\n",
    "# create a virtual environment\n",
    "python3 -m venv venv-jumpstart-lab1\n",
    "# activate the virtual environment\n",
    "source venv-jumpstart-lab1/bin/activate\n",
    "### COPY TO HERE ONLY IF RUNNING AS ONE COPY AND PASTE ###\n",
    "\n",
    "### STOP ###\n",
    "### MAKE SURE ABOVE VENV GETS ACTIVATED BEFORE RUNNING THE REST ###\n",
    "# upgrade pip\n",
    "pip install --upgrade pip\n",
    "# jupyter kernel support\n",
    "pip install ipykernel\n",
    "# add the virtual environment to jupyter\n",
    "python  -m ipykernel install --user --name=venv-jumpstart-lab1 --display-name \"Python (venv-jumpstart-lab1)\"\n",
    "# install the required packages\n",
    "pip install -r Documents/github/labs-sagemaker/jumpstart/requirements_lab1.txt\n",
    "# verify the installation\n",
    "pip list\n",
    "\n",
    "### RESTART VSCODE TO PICKUP THE NEW VENV ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcb8f1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "### STOP ###\n",
    "### This command is for activating an environment that already exists, its for use in a terminal window if you need it ###\n",
    "source venv-jumpstart-lab1/bin/activate\n",
    "pip list\n",
    "\n",
    "# use pip freeze if you prefer for requirements.txt freiendly format\n",
    "### ALSO MAKE SURE YOU SELECT IT AS YOUR KERNEL FOR THIS JUPYTER NOTEBOOK ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c70f3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "### STOP ###\n",
    "### IF USING THIS NOTEBOOK IN A SAGEMAKER JUPYTER NOTEBOOK INSTANCE, THEN EXECUTE THIS CELL ###\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install --upgrade locust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb8cd5",
   "metadata": {},
   "source": [
    "# Lab 1 Starts Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86ebb0",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Setup</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40fc3db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# region\n",
    "# for the purpose of this lab, us-east-1, us-west-2, eu-west-1 has the broadest coverage of JumpStart models and instance types\n",
    "# if you provision in other regions, you may not have access to all the models or instance types, and may need to request increase of quotas for some instance types\n",
    "myRegion='us-east-1'\n",
    "\n",
    "# iam\n",
    "myRoleSageMakerExecution=\"doit-jumpstart-sagemaker-execution-role\"\n",
    "myRoleSageMakerExecutionARN='RETRIEVED FROM ROLE BELOW'\n",
    "\n",
    "# parameter store\n",
    "myParameterStoreEndpointName='doit-jumpstart-sagemaker-endpoint-name'\n",
    "myParameterStoreIAMARN='doit-jumpstart-sagemaker-iam-arn'\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb4a28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546709318047\n",
      "arn:aws:iam::546709318047:user/simon-davies-cli\n",
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from certifi import where\n",
    "\n",
    "botoSession = boto3.Session(region_name=myRegion)\n",
    "\n",
    "# Configure boto3 to use certifi's certificates - helps avoid SSL errors if your system’s certificate store is out of date or missing root certs\n",
    "sts_client = boto3.client('sts', verify=where())\n",
    "myAccountNumber = sts_client.get_caller_identity()[\"Account\"]\n",
    "print(myAccountNumber)\n",
    "print(sts_client.get_caller_identity()[\"Arn\"])\n",
    "\n",
    "# create clients we can use later\n",
    "# iam\n",
    "iam = boto3.client('iam', region_name=myRegion, verify=where())\n",
    "# ssm\n",
    "ssm = boto3.client('ssm', region_name=myRegion, verify=where())\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01b6da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# define tags added to all services we create\n",
    "myTags = [\n",
    "    {\"Key\": \"env\", \"Value\": \"non_prod\"},\n",
    "    {\"Key\": \"owner\", \"Value\": \"doit-jumpstart\"},\n",
    "    {\"Key\": \"project\", \"Value\": \"lab1\"},\n",
    "    {\"Key\": \"author\", \"Value\": \"simon\"},\n",
    "]\n",
    "myTagsDct = {\n",
    "    \"env\": \"non_prod\",\n",
    "    \"owner\": \"doit-jumpstart\",\n",
    "    \"project\": \"lab1\",\n",
    "    \"author\": \"simon\",\n",
    "}\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67085b",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">IAM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9941d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSageMakerExecutionRole():\n",
    "    \"\"\"\n",
    "    Creates a role required for SageMaker to run jobs on your behalf\n",
    "    Only needed if this is being run in a local IDE, not needed if in SageMaker Studio or SageMaker Notebook Instance\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        An IAM execution role ARN\n",
    "    \"\"\"\n",
    "\n",
    "    # trust policy for the role\n",
    "    roleTrust = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"sagemaker.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # check if the role exists\n",
    "    try:\n",
    "        role = iam.get_role(RoleName=myRoleSageMakerExecution)\n",
    "        print(\"Role already exists. Using the existing role.\")\n",
    "        return role['Role']['Arn']\n",
    "    except iam.exceptions.NoSuchEntityException:\n",
    "        print(\"Role does not exist. Creating a new role.\")\n",
    "        \n",
    "    # create execution role for sagemaker - allows SageMaker notebook instances, training jobs, and models to access S3, ECR, and CloudWatch on your behalf\n",
    "    # this role is only created if we are running this notebook in a local ide, if we are in a jupyterlab in sagemaker studio, we dont need it as already created and available\n",
    "    role = iam.create_role(\n",
    "        RoleName=myRoleSageMakerExecution,\n",
    "        AssumeRolePolicyDocument=json.dumps(roleTrust),\n",
    "        Description=\"Service excution role for sagemaker ai use including inside jupyter notebooks\",\n",
    "        Tags=[\n",
    "            *myTags,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # attach managed policy to the role AmazonSageMakerFullAccess\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=myRoleSageMakerExecution,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n",
    "    )\n",
    "\n",
    "    # store the role arn in parameter store for use in other notebooks\n",
    "    ssm.put_parameter(\n",
    "        Name=myParameterStoreIAMARN,\n",
    "        Description='The ARN of the IAM role used by SageMaker for execution of jobs',\n",
    "        Value=role['Role']['Arn'],\n",
    "        Type='String',\n",
    "        Tags=[\n",
    "            *myTags,\n",
    "        ],\n",
    "    )   \n",
    "\n",
    "    return role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708857ff",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get Execution Role and Session</span>\n",
    "- SageMaker requires an execution role to assume on your behalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80e08fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name simon-davies-cli to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role does not exist. Creating a new role.\n",
      "arn:aws:iam::546709318047:role/doit-jumpstart-sagemaker-execution-role\n",
      "<sagemaker.session.Session object at 0x108f0b9b0>\n",
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "try:\n",
    "    # if this is being run in a SageMaker AI JupyterLab Notebook\n",
    "    myRoleSageMakerExecutionARN = get_execution_role()\n",
    "except:\n",
    "    # if this is being run in a local IDE - we need to create our own role\n",
    "    myRoleSageMakerExecutionARN = getSageMakerExecutionRole()\n",
    "\n",
    "# make sure we get a session in the correct region (needed as it can use the aws configure region if running this locally\n",
    "sageMakerSession = Session(boto_session=botoSession)\n",
    "\n",
    "print(myRoleSageMakerExecutionARN)\n",
    "print(sageMakerSession)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56ace1",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Provision a JumpStart Model</span>\n",
    "- Provision a model via Jumpstart\n",
    "- If you prefer, you can also do this via the JumpStart console, but you will have to bring in the endpoint that you create to continue with this code\n",
    "### Example models to provision\n",
    "- Qwen2.5 Coder 32B Instruct\n",
    "  - *model_id, model_version = \"huggingface-llm-qwen2-5-coder-32b-instruct\", \"*\"*\n",
    "  - this model requires an instance needs with multiple GPUs\n",
    "    - a single GPU will fail with an error like \"AssertionError: Each process is one gpu\"\n",
    "  - requires a huge instance to run, eg \n",
    "    - ml.g6.48xlarge (low-cost inference at scale, not training)\n",
    "      - **needs an aws quota increase for this instance**\n",
    "    - ml.p4de.24xlarge or ml.p5.48xlarge (alternatives maximum raw performance / less headache, go)\n",
    "      - **needs an aws quota increase for this instance**\n",
    "    - ml.g5.48xlarge\n",
    "      - good for a poc - widely supported, good memory, reasonably costed\n",
    "      - good for inference of 32B models with tensor parallelism (JumpStart handles this automatically when deploying with multiple GPUs)\n",
    "      - **needs an aws quota increase for this instance**\n",
    "- see https://aws.amazon.com/sagemaker/ai/pricing/ for pricing, **larger instances can be very expensive per hour**\n",
    "- If you deply the model and you get a quota error, you will need to visit Service Quotas via the console and request an increase\n",
    "  - go to SageMaker service and search for the instance\n",
    "  - make sure your quota allows for auto scaling max\n",
    "- DO NOT LEAVE LARGE INSTANCES RUNNING LONGER THAN YOU NEED TO $$$!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dde9012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an option:\n",
      "1. huggingface-llm-qwen2-5-coder-32b-instruct|ml.g5.48xlarge $$$$!\n",
      "2. meta-textgeneration-llama-2-7b|ml.g5.4xlarge $$\n",
      "3. model-txt2img-stabilityai-stable-diffusion-v2|ml.g5.4xlarge $$\n",
      "You selected: meta-textgeneration-llama-2-7b on ml.g5.4xlarge\n",
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# the model we want to provision - THIS DISPLAYS AN INPUT BOX FOR YOU TO CHOOSE A MODEL\n",
    "# jump into the console, click on JumpStart, find a model you like and copy the model id from the details page\n",
    "# https://aws.amazon.com/sagemaker/ai/pricing/\n",
    "options = [\"huggingface-llm-qwen2-5-coder-32b-instruct|ml.g5.48xlarge $$$$!\", \"meta-textgeneration-llama-2-7b|ml.g5.4xlarge $$\", \"model-txt2img-stabilityai-stable-diffusion-v2|ml.g5.4xlarge $$\"]\n",
    "\n",
    "print(\"Select an option:\")\n",
    "for i, opt in enumerate(options, 1):\n",
    "    print(f\"{i}. {opt}\")\n",
    "\n",
    "choice = int(input(\"Enter the number of the model you want: \"))\n",
    "selected = options[choice - 1]\n",
    "\n",
    "modelID = selected.split('|')[0]\n",
    "instanceType = selected.split('|')[1].split(' ')[0]\n",
    "\n",
    "print(f\"You selected: {modelID} on {instanceType}\")\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0dffb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgeneration-llama-2-7b' with wildcard version identifier '*'. You can pin to version '4.19.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "# if your selected model is gated, you will need to set accept_eula to True to accept the model end-user license agreement (EULA)\n",
    "accept_eula = True\n",
    "\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.jumpstart.model.JumpStartModel\n",
    "model = JumpStartModel(\n",
    "    model_id=modelID,\n",
    "    model_version=\"*\",\n",
    "    instance_type=instanceType,      \n",
    "    role=myRoleSageMakerExecutionARN,\n",
    "    sagemaker_session=sageMakerSession,\n",
    "    region=myRegion\n",
    ")\n",
    "\n",
    "# deploy the model to an endpoint\n",
    "# this will take a while\n",
    "predictor = model.deploy(accept_eula=accept_eula, initial_instance_count=1)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "583398e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# store the predictor name in a parameter store for use in other notebooks\n",
    "ssm.put_parameter(\n",
    "    Name=myParameterStoreEndpointName,\n",
    "    Description='the name of the sagemaker endpoint created in lab1',\n",
    "    Value=predictor.endpoint_name,\n",
    "    Type='String',\n",
    "    Tags=[\n",
    "        *myTags,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f31cd654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " {'inputs': 'import socket\\n\\ndef ping_exponential_backoff(host: str):', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.2, 'decoder_input_details': True, 'details': True}}\n",
      "\n",
      "Output:\n",
      " import socket\n",
      "\n",
      "def ping_exponential_backoff(host: str):\n",
      "    \"\"\"\n",
      "    This function is used to ping a host and return the response time.\n",
      "    :param host: host to ping\n",
      "    :return: response time\n",
      "    \"\"\"\n",
      "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "    s.connect((host, 80))\n",
      "    s.close()\n",
      "    return s.gettimeout()\n",
      "\n",
      "def ping_exponential_backoff_with_timeout(host: str, timeout: int):\n",
      "    \"\"\"\n",
      "    This function is used to ping a host and return the response time.\n",
      "    :param host: host to ping\n",
      "    :param timeout: timeout in seconds\n",
      "    :return: response time\n",
      "    \"\"\"\n",
      "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "    s.connect((host, 80))\n",
      "    s.close()\n",
      "    return s.gettimeout()\n",
      "\n",
      "def ping_exponential_backoff_with_timeout_and_retry(host: str, timeout: int, retry: int):\n",
      "    \"\"\"\n",
      "    This function is used to ping a host\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': 'import argparse\\n\\ndef main(string: str):\\n    print(string)\\n    print(string[::-1])\\n\\nif __name__ == \"__main__\":', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.05}}\n",
      "\n",
      "Output:\n",
      " import argparse\n",
      "\n",
      "def main(string: str):\n",
      "    print(string)\n",
      "    print(string[::-1])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\"string\", type=str, help=\"string to reverse\")\n",
      "    args = parser.parse_args()\n",
      "    main(args.string)\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': 'def fib(n):\\n', 'parameters': {'max_new_tokens': 64, 'top_p': 0.9, 'temperature': 0.2, 'decoder_input_details': True, 'details': True}}\n",
      "\n",
      "Output:\n",
      " def fib(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "def fib_rec(n):\n",
      "    if n == 0:\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': 'def remove_non_ascii(s: str) -> str:\\n    \"\"\"<FILL>\\n    return result\\n', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.05, 'decoder_input_details': True, 'details': True}}\n",
      "\n",
      "Output:\n",
      " def remove_non_ascii(s: str) -> str:\n",
      "    \"\"\"<FILL>\n",
      "    return result\n",
      "    \"\"\"\n",
      "    return s.translate(str.maketrans(\"\", \"\", string.ascii_letters + string.digits))\n",
      "\n",
      "\n",
      "def remove_non_ascii_from_list(s: list) -> list:\n",
      "    \"\"\"<FILL>\n",
      "    return result\n",
      "    \"\"\"\n",
      "    return [remove_non_ascii(s) for s in s]\n",
      "\n",
      "\n",
      "def remove_non_ascii_from_dict(s: dict) -> dict:\n",
      "    \"\"\"<FILL>\n",
      "    return result\n",
      "    \"\"\"\n",
      "    return {k: remove_non_ascii(v) for k, v in s.items()}\n",
      "\n",
      "\n",
      "def remove_non_ascii_from_tuple(s: tuple) -> tuple:\n",
      "    \"\"\"<FILL>\n",
      "    return result\n",
      "    \"\"\"\n",
      "    return tuple(remove_non_ascii(v) for v in s)\n",
      "\n",
      "\n",
      "def remove_non_ascii_from_set(s: set) -> set:\n",
      "    \"\"\"<FILL>\n",
      "    return result\n",
      "    \"\"\"\n",
      "    return {remove_non_ascii(v) for v in s}\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': '# Installation instructions:\\n    ```bash\\n<FILL>\\n    ```\\nThis downloads the LLaMA inference code and installs the repository as a local pip package.\\n', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.05}}\n",
      "\n",
      "Output:\n",
      " # Installation instructions:\n",
      "    ```bash\n",
      "<FILL>\n",
      "    ```\n",
      "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
      "\n",
      "### Installation instructions:\n",
      "\n",
      "```bash\n",
      "<FILL>\n",
      "```\n",
      "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
      "\n",
      "### Installation instructions:\n",
      "\n",
      "```bash\n",
      "<FILL>\n",
      "```\n",
      "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
      "\n",
      "### Installation instructions:\n",
      "\n",
      "```bash\n",
      "<FILL>\n",
      "```\n",
      "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
      "\n",
      "### Installation instructions:\n",
      "\n",
      "```bash\n",
      "<FILL>\n",
      "```\n",
      "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
      "\n",
      "### Installation instructions:\n",
      "\n",
      "```bash\n",
      "<FILL>\n",
      "```\n",
      "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
      "\n",
      "### Installation instructions:\n",
      "\n",
      "```bash\n",
      "<FILL>\n",
      "```\n",
      "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
      "\n",
      "### Installation instructions:\n",
      "\n",
      "```bash\n",
      "<FILL>\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': 'class InterfaceManagerFactory(AbstractManagerFactory):\\n    def __init__(<FILL>\\ndef main():\\n    factory = InterfaceManagerFactory(start=datetime.now())\\n    managers = []\\n    for i in range(10):\\n        managers.append(factory.build(id=i))\\n', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.05}}\n",
      "\n",
      "Output:\n",
      " class InterfaceManagerFactory(AbstractManagerFactory):\n",
      "    def __init__(<FILL>\n",
      "def main():\n",
      "    factory = InterfaceManagerFactory(start=datetime.now())\n",
      "    managers = []\n",
      "    for i in range(10):\n",
      "        managers.append(factory.build(id=i))\n",
      "    for manager in managers:\n",
      "        manager.start()\n",
      "    for manager in managers:\n",
      "        manager.stop()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "\n",
      "Input:\n",
      " {'inputs': '/-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\\ntheorem connected_iff_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\\n  π₁ P = 0 ↔ <FILL> = 0 :=\\nbegin\\n  split,\\n  { intros h f,\\n    rw pi_1_etalisation at h,\\n    simp [h],\\n    refl\\n  },\\n  { intro h,\\n    have := @quasi_adjoint C D P,\\n    simp [←pi_1_etalisation, this, h],\\n    refl\\n  }\\nend\\n', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.05}}\n",
      "\n",
      "Output:\n",
      " /-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\n",
      "theorem connected_iff_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\n",
      "  π₁ P = 0 ↔ <FILL> = 0 :=\n",
      "begin\n",
      "  split,\n",
      "  { intros h f,\n",
      "    rw pi_1_etalisation at h,\n",
      "    simp [h],\n",
      "    refl\n",
      "  },\n",
      "  { intro h,\n",
      "    have := @quasi_adjoint C D P,\n",
      "    simp [←pi_1_etalisation, this, h],\n",
      "    refl\n",
      "  }\n",
      "end\n",
      "\n",
      "/-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\n",
      "theorem connected_iff_etalisation_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\n",
      "  π₁ P = 0 ↔ <FILL> = 0 :=\n",
      "begin\n",
      "  split,\n",
      "  { intros h f,\n",
      "    rw pi_1_etalisation_etalisation at h,\n",
      "    simp [h],\n",
      "    refl\n",
      "  },\n",
      "  { intro h,\n",
      "    have := @quasi_adjoint C D P,\n",
      "    simp [←pi_1_etalisation_etalisation, this, h],\n",
      "    refl\n",
      "  }\n",
      "end\n",
      "\n",
      "/-- A quasi-prefunctoid is 1-connected iff all its etalisations are 1-connected. -/\n",
      "theorem connected_iff_etalisation_etalisation_etalisation [C D : precategoroid] (P : quasi_prefunctoid C D) :\n",
      "  π₁ P = 0 ↔ <FILL> = 0 :=\n",
      "begin\n",
      "\n",
      "\n",
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# test the endpoint with some example payloads\n",
    "example_payloads = model.retrieve_all_examples()\n",
    "\n",
    "if example_payloads is None:\n",
    "    print(\"Using your own as no example models available for this model.\")\n",
    "    # try your own payload\n",
    "    example_payloads = [\n",
    "        {\n",
    "            \"body\": {\n",
    "                \"inputs\": \"Write a Python function to check if a number is prime\",\n",
    "                \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.2, \"top_p\": 0.9},\n",
    "            },\n",
    "            \"content_type\": \"application/json\",\n",
    "            \"accept\": \"application/json\",\n",
    "        },\n",
    "        {\n",
    "            \"body\": {\n",
    "                \"inputs\": \"Describe what a llm model can do for someone who is sceptical about them\",\n",
    "                \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.2, \"top_p\": 0.9},\n",
    "            },\n",
    "            \"content_type\": \"application/json\",\n",
    "            \"accept\": \"application/json\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "for payload in example_payloads:\n",
    "    body = payload.body if hasattr(payload, \"body\") else payload[\"body\"]\n",
    "    response = predictor.predict(body)\n",
    "    response = response[0] if isinstance(response, list) else response\n",
    "    print(\"Input:\\n\", body, end=\"\\n\\n\")\n",
    "    print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfc496",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Move to Lab 2</span>\n",
    "# <span style=\"color:DarkSeaGreen\">OR...</span>\n",
    "# <span style=\"color:DarkSeaGreen\">Clean Up Architecture</span>\n",
    "### <span style=\"color:Red\">Only do this if you have finished with this lab and any labs that depend on it!</span>\n",
    "##### It will delete all architecture created, make sure you no longer need any of it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60e2ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# when finished with the endpoint, delete it\n",
    "predictor.delete_predictor()\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ced04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# delete roles and policies\n",
    "iam.detach_role_policy(\n",
    "    RoleName=myRoleSageMakerExecution, PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    ")\n",
    "iam.delete_role(RoleName=myRoleSageMakerExecution)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa145c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Move to the next cell ->\n"
     ]
    }
   ],
   "source": [
    "# delete the parameter store entry\n",
    "ssm.delete_parameter(Name=myParameterStoreEndpointName)\n",
    "ssm.delete_parameter(Name=myParameterStoreIAMARN)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05607d8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Clean Up venv</span>\n",
    "### Clean up if finished with this lab and running in VSCode or equivalent local IDE\n",
    "#### Note these are macOS specific\n",
    "- Run the commands of the cell below in a terminal window if you need to clean up a local venv\n",
    "  - Note if you copy and paste the entire cell and run as one you will get zsh: command not found: # errors because of the comments, but you can ignore\n",
    "  - Remember to restart the kernel to refresh whats available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89229fe8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# deactivate the venv\n",
    "deactivate \n",
    "# remove it and its contents if not needed\n",
    "rm -rf venv-jumpstart-lab1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
