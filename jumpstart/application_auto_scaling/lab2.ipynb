{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06920b65",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:DarkSeaGreen\">JumpStart Lab 2</span>\n",
    "\n",
    "This lab does the following:\n",
    "\n",
    "- Uses the endpoint created in Lab 1\n",
    "- Implements SageMaker application-autoscaling **target tracking** policy\n",
    "- Tests the functionality \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb29b8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Prepare Your Environment</span>\n",
    "### Note if you want a venv, see Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb8cd5",
   "metadata": {},
   "source": [
    "# Lab 2 Starts Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86ebb0",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Setup</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region\n",
    "# for the purpose of this lab, us-east-1, us-west-2, eu-west-1 has the broadest coverage of JumpStart models and instance types\n",
    "# if you provision in other regions, you may not have access to all the models or instance types, and may need to request increase of quotas for some instance types\n",
    "myRegion='us-east-1'\n",
    "\n",
    "# parameter store\n",
    "myParameterStoreChosenModel='doit-jumpstart-sagemaker-chosen-model'\n",
    "myParameterStoreEndpointName='doit-jumpstart-sagemaker-endpoint-name'\n",
    "myParameterStoreIAMARN='doit-jumpstart-sagemaker-iam-arn'\n",
    "\n",
    "# application auto scaling policy\n",
    "myEndpointScalingPolicyName='doit-jumpstart-sagemaker-endpoint-target-tracking-scaling-policy'\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "from certifi import where\n",
    "\n",
    "botoSession = boto3.Session(region_name=myRegion)\n",
    "\n",
    "# Configure boto3 to use certifi's certificates - helps avoid SSL errors if your system’s certificate store is out of date or missing root certs\n",
    "sts_client = boto3.client('sts', verify=where())\n",
    "myAccountNumber = sts_client.get_caller_identity()[\"Account\"]\n",
    "print(myAccountNumber)\n",
    "print(sts_client.get_caller_identity()[\"Arn\"])\n",
    "\n",
    "# create clients we can use later\n",
    "# iam\n",
    "iam = boto3.client('iam', region_name=myRegion, verify=where())\n",
    "# ssm\n",
    "ssm = boto3.client('ssm', region_name=myRegion, verify=where())\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tags added to all services we create\n",
    "myTags = [\n",
    "    {\"Key\": \"env\", \"Value\": \"non_prod\"},\n",
    "    {\"Key\": \"owner\", \"Value\": \"doit-jumpstart\"},\n",
    "    {\"Key\": \"project\", \"Value\": \"lab1\"},\n",
    "    {\"Key\": \"author\", \"Value\": \"simon\"},\n",
    "]\n",
    "myTagsDct = {\n",
    "    \"env\": \"non_prod\",\n",
    "    \"owner\": \"doit-jumpstart\",\n",
    "    \"project\": \"lab1\",\n",
    "    \"author\": \"simon\",\n",
    "}\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a8523",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">IAM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSageMakerExecutionRole():\n",
    "    \"\"\"\n",
    "    Gets a role required for SageMaker to run jobs on your behalf\n",
    "    Only needed if this is being run in a local IDE, not needed if in SageMaker Studio or SageMaker Notebook Instance\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        An IAM execution role ARN\n",
    "    \"\"\"\n",
    "\n",
    "    # get the role we created in the previous lab from the parameter store\n",
    "    response = ssm.get_parameter(Name=myParameterStoreIAMARN)\n",
    "    myRoleSageMakerExecutionARN = response['Parameter']['Value']\n",
    "    print(f\"Retrieved role from parameter store: {myRoleSageMakerExecutionARN}\")    \n",
    "\n",
    "    return myRoleSageMakerExecutionARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708857ff",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get Execution Role and Session</span>\n",
    "- SageMaker requires an execution role to assume on your behalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e08fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "try:\n",
    "    # if this is being run in a SageMaker AI JupyterLab Notebook\n",
    "    myRoleSageMakerExecutionARN = get_execution_role()\n",
    "except:\n",
    "    # if this is being run in a local IDE - we need to create our own role\n",
    "    myRoleSageMakerExecutionARN = getSageMakerExecutionRole()\n",
    "\n",
    "# make sure we get a session in the correct region (needed as it can use the aws configure region if running this locally\n",
    "sageMakerSession = Session(boto_session=botoSession)\n",
    "\n",
    "print(myRoleSageMakerExecutionARN)\n",
    "print(sageMakerSession)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3553bb10",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get the Endpoint from Lab 1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcada497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the endpoint created in lab1\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# get the endpoint name from parameter store\n",
    "response = ssm.get_parameter(\n",
    "    Name=myParameterStoreEndpointName\n",
    ")\n",
    "endpointName = response['Parameter']['Value']\n",
    "print(f\"Using endpoint name: {endpointName}\")  \n",
    "\n",
    "# create a predictor to interact with the endpoint - need to specify the default serializer and deserializer this time\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpointName,\n",
    "    sagemaker_session=sageMakerSession,\n",
    "    serializer=JSONSerializer(),      \n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33feeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required if an image model is being used\n",
    "def decode_and_show(model_response) -> None:\n",
    "    from PIL import Image\n",
    "    import base64\n",
    "    import io\n",
    "    \n",
    "    image = Image.open(io.BytesIO(base64.b64decode(model_response)))\n",
    "    display(image)\n",
    "    image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the endpoint\n",
    "import random\n",
    "\n",
    "# get the model we deployed from parameter store\n",
    "response = ssm.get_parameter(\n",
    "    Name=myParameterStoreChosenModel\n",
    ")\n",
    "chosenModel = response['Parameter']['Value']\n",
    "modelType = chosenModel.split(\"|\")[0]\n",
    "modelID = chosenModel.split(\"|\")[1]\n",
    "instanceType = chosenModel.split(\"|\")[2].split(\" \")[0]\n",
    "print(f\"You selected: model type {modelType} {modelID} on {instanceType}\")\n",
    "\n",
    "if modelType == \"llm\":\n",
    "    example_payloads = [\n",
    "        {\n",
    "            \"body\": {\n",
    "                \"inputs\": \"Describe what a llm model can do for someone who is sceptical about them\",\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 128,\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"top_p\": 0.9,\n",
    "                },\n",
    "            },\n",
    "            \"content_type\": \"application/json\",\n",
    "            \"accept\": \"application/json\",\n",
    "        },\n",
    "    ]\n",
    "else:\n",
    "    example_payloads = [\n",
    "        {\n",
    "            \"text_prompts\": [\n",
    "                {\n",
    "                    \"text\": \"A cowboy standoff at sunset in a dusty desert town, cinematic wide shot, golden hour lighting, photorealistic\"\n",
    "                }\n",
    "            ],\n",
    "            \"width\": 512,\n",
    "            \"height\": 512,\n",
    "            \"cfg_scale\": 7.0,\n",
    "            \"steps\": 150,\n",
    "            \"seed\": random.randint(0, 4294967295),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "for payload in example_payloads:\n",
    "    if modelType == \"llm\":\n",
    "        body = payload.body if hasattr(payload, \"body\") else payload[\"body\"]\n",
    "        response = predictor.predict(body)\n",
    "        response = response[0] if isinstance(response, list) else response\n",
    "        print(\"Input:\\n\", body, end=\"\\n\\n\")\n",
    "        print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n",
    "    else:\n",
    "        #payload = json.dumps(payload).encode(\"utf-8\") # in case you change the image model that needs it\n",
    "        response = predictor.predict(payload)\n",
    "        decode_and_show(response[\"generated_image\"])\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e923450",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Create Scalability Plan - Target Tracking</span>\n",
    "- Uses SageMaker Application Auto Scaling - Target Tracking\n",
    "- A target tracking scaling policy automatically scales your application based on a target metric value. This allows your application to maintain optimal performance and cost efficiency without manual intervention.\n",
    "- With target tracking, you select a metric and a target value to represent the ideal average utilization or throughput level for your application. Application Auto Scaling creates and manages the CloudWatch alarms that trigger scaling events when the metric deviates from the target. This is similar to how a thermostat maintains a target temperature.\n",
    "- Using the new high-resolution metrics allow you to greatly decrease the time it takes to scale up an endpoint using Application Auto Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef977508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/huggingfacetgi/meta-llama/llama3-8b/faster-autoscaling/realtime-endpoints/FasterAutoscaling-SME-Llama3-8B-AppAutoScaling.ipynb\n",
    "# https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-launches-faster-auto-scaling-for-generative-ai-models/\n",
    "# https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html\n",
    "\n",
    "# define a new auto scaling target for Application Auto Scaling\n",
    "# we will use target tracking scaling - scale a resource based on a target value for a specific CloudWatch metric\n",
    "# auto scaling\n",
    "autoScaling = boto3.client('application-autoscaling', region_name=myRegion, verify=where())\n",
    "variantName = \"AllTraffic\"\n",
    "ResourceId  = \"endpoint/\" + endpointName + \"/variant/\" + variantName\n",
    "\n",
    "# Register scalable target\n",
    "scalableTarget = autoScaling.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=ResourceId,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5,  # Replace with your desired maximum instances\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target tracking scaling policy\n",
    "# this is a target tracking policy that uses the new high-resolution metrics for SageMaker endpoints\n",
    "# you can also create a step-scaling policy if you prefer\n",
    "# a step-scaling policy is more complex to set up, but gives you more control over how your endpoint scales\n",
    "\n",
    "# create a policy that scales out when the endpoint receives more than n ConcurrentRequestsPerModel\n",
    "# this new metric will be tracked when the predefined metric type used below is SageMakerVariantConcurrentRequestsPerModelHighResolution\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/application-autoscaling/client/put_scaling_policy.html\n",
    "# NOTE this creates alarms with default thresholds, you can modify these alarms after creation if you want to change the thresholds\n",
    "# Eg 90 datapoints within 15 minutes\" is the default conservative behavior for scale-in to prevent flapping\n",
    "# you can change this to a more aggressive scale-in policy if you want to scale in faster by modifying the alarm created though not recommended\n",
    "# AWS automatically sets the scale-out and scale-in thresholds based on the TargetValue you specify in your target tracking policy\n",
    "# these thresholds determine when your endpoint variant scales up or down\n",
    "# sageMaker automatically populates the metric data for your endpoint, so no manual configuration is needed.\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-add-code-define.html#endpoint-auto-scaling-add-code-high-res\n",
    "# other auto application tarfet tracking scaling predefined metric types you can use are\n",
    "# - https://docs.aws.amazon.com/autoscaling/application/userguide/monitoring-cloudwatch.html#predefined-metrics\n",
    "# - SageMakerVariantInvocationsPerInstance\n",
    "# - SageMakerVariantProvisionedConcurrencyUtilization\n",
    "# - SageMakerInferenceComponentInvocationsPerCopy\n",
    "# - SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\n",
    "# - SageMakerVariantConcurrentRequestsPerModelHighResolution\n",
    "targetTrackingPolicyResponse = autoScaling.put_scaling_policy(\n",
    "    PolicyName=myEndpointScalingPolicyName,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=ResourceId,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 55.0,  # Scaling triggers when endpoint receives n ConcurrentRequestsPerModel\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantConcurrentRequestsPerModelHighResolution\"\n",
    "        },\n",
    "        \"ScaleInCooldown\": 60,  # Cooldown period after scale-in activity - this means it will scale 1 instance IN every n seconds if the metric is below the threshold\n",
    "        \"ScaleOutCooldown\": 120,  # Cooldown period after scale-out activity - this means it will scale 1 instance OUT every n seconds if the metric is above the threshold\n",
    "    },\n",
    ")\n",
    "\n",
    "# print(target_tracking_policy_response)\n",
    "print(f\"[b]Policy ARN:[/b] [i blue]{targetTrackingPolicyResponse['PolicyARN']}\")\n",
    "\n",
    "# print Cloudwatch Alarms\n",
    "alarms = targetTrackingPolicyResponse[\"Alarms\"]\n",
    "\n",
    "for alarm in alarms:\n",
    "    print(f\"[b]Alarm Name:[/b] [b magenta]{alarm['AlarmName']}\")\n",
    "    # print(f\"[b]Alarm ARN:[/b] [i green]{alarm['AlarmARN']}[/i green]\")\n",
    "    print(\"===\" * 15)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece097a3",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Test Scalability Plan</span>\n",
    "- Lets just test the endpoint first, make sure its all good\n",
    "- Simulate load\n",
    "- Make sure your venv is activated, eg\n",
    "  - activate the virtual environment\n",
    "  - source venv-jumpstart-lab1/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to use locust to simulate load on the endpoint\n",
    "# https://docs.locust.io/en/stable/ \n",
    "# https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/\n",
    "# see the locust_script_lab2.py file for details of the load test\n",
    "# it gathers the endpoint name, etc via os environment vars we export below\n",
    "# run this cell, then paste and run in a terminal window, make sure its run in your virtual environment created in lab 1, or in your own that has boto3 and locust installed\n",
    "\n",
    "print (\"export AWS_REGION={}\".format(myRegion))\n",
    "print (\"export ENDPOINT_NAME={}\".format(endpointName))\n",
    "print (\"export CONTENT_TYPE={}\".format(\"application/json\"))\n",
    "print (\"export MODEL_TYPE={}\".format(modelType))\n",
    "print (\"export HOST={}\".format('http://localhost'))\n",
    "if modelType == \"llm\":\n",
    "    print (\"export PAYLOAD='{}'\".format('{\"inputs\": \"Please explain what load testing is and why its important in reference to sagemaker endpoints\"}'))\n",
    "else:\n",
    "    print (\"export PAYLOAD='{}'\".format('A cowboy standoff at sunset in a dusty desert town, cinematic wide shot, golden hour lighting, photorealistic'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94932b41",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# these are picked up by the locust file\n",
    "# Paste the following in a terminal window, make sure its run in your virtual environment created in lab 1, or in your own that has boto3 and locust installed\n",
    "# LOCUST_USERS is the number of simulated users\n",
    "# LOCUST_SPAWN_RATE is the rate per second to spawn (add new) users - so 20 users at rate of 2 means add 2 users every second, so take 10 seconds to get to 20 users\n",
    "# LOCUST_RUN_TIME is how long to run the test for\n",
    "export LOCUST_USERS=140\n",
    "export LOCUST_SPAWN_RATE=0.5\n",
    "export LOCUST_RUN_TIME=15m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be23fb",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Before Locust is Run</span>\n",
    "- Go to the CloudWatch console\n",
    "- Monitor the alarm being target tracked for ConcurrentRequestsPerModel, eg \n",
    "  - TargetTracking-endpoint/*endpoint name*-Alarm**High**-*uuid*\n",
    "  - TargetTracking-endpoint/*endpoint name*-Alarm**Low**-*uuid*\n",
    "- Run the cell below to monitor the instance count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "endpoint_name = endpointName\n",
    "region = myRegion\n",
    "poll_interval = 10    # seconds between checks\n",
    "\n",
    "# --- Clients ---\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "cw_client = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "print(f\"Monitoring endpoint '{endpoint_name}' variants (press Ctrl+C to stop)...\\n\")\n",
    "print(\"Alarms for reference:\")\n",
    "print(f\"High Scaling Alarm: {alarms[0]['AlarmName']} | Scale cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when above {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n",
    "print(f\"Low Scaling Alarm: {alarms[1]['AlarmName']} | Scale cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when below {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # --- Describe endpoint variants ---\n",
    "        response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        for variant in response[\"ProductionVariants\"]:\n",
    "            variant_name = variant[\"VariantName\"]\n",
    "            current_instances = variant[\"CurrentInstanceCount\"]\n",
    "            desired_instances = variant[\"DesiredInstanceCount\"]\n",
    "\n",
    "            # --- Fetch latest ConcurrentRequestsPerModel metric ---\n",
    "            end_time = datetime.utcnow()\n",
    "            start_time = end_time - timedelta(seconds=poll_interval*2)  # small window to get the latest datapoint\n",
    "\n",
    "            # get the ConcurrentRequestsPerModel metric for this variant\n",
    "            metric_resp = cw_client.get_metric_statistics(\n",
    "                Namespace=\"AWS/SageMaker\",\n",
    "                MetricName=\"ConcurrentRequestsPerModel\",\n",
    "                Dimensions=[\n",
    "                    {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                    {\"Name\": \"VariantName\", \"Value\": variant_name},\n",
    "                ],\n",
    "                StartTime=start_time,\n",
    "                EndTime=end_time,\n",
    "                Period=poll_interval,\n",
    "                Statistics=[\"Average\"],\n",
    "            )\n",
    "\n",
    "            datapoints = metric_resp.get(\"Datapoints\", [])\n",
    "            concurrent_requests = round(datapoints[-1][\"Average\"], 2) if datapoints else 0\n",
    "\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Variant: {variant_name} | \"\n",
    "                f\"Current instances: {current_instances} | Desired instances: {desired_instances} | \"\n",
    "                f\"ConcurrentRequestsPerModel: {concurrent_requests} | \"\n",
    "                f\"High Scaling Alarm State: {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['StateValue']} | \"\n",
    "                f\"Low Scaling Alarm State: {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['StateValue']}\"\n",
    "            )\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Monitoring stopped.\")\n",
    "    # print each alarm name and description on a new line\n",
    "    print(\"Alarms for reference:\")\n",
    "    print(f\"High Scaling Alarm: {alarms[1]['AlarmName']} | Scale cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when above {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[0]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n",
    "    print(f\"Low Scaling Alarm: {alarms[1]['AlarmName']} | Scale cooldown (seconds): {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period'] * cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} | when below {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Threshold']} for {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['EvaluationPeriods']} periods of {cw_client.describe_alarms(AlarmNames=[alarms[1]['AlarmName']])['MetricAlarms'][0]['Period']} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d9307",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Start Locust</span>\n",
    "- Paste the command into your terminal window in your venv\n",
    "- If you are testing load with a txt2img model on a mac, and you see assertion errors for threading\n",
    "  - this is a harmless warning related to gevent (the async library Locust uses) and Python's threading system and can be ignored\n",
    "\n",
    "## # fails\n",
    "- if you see a failure rate more than 0%, or it climbs as user load increases its likely you do not have instance sizes that can handle the load\n",
    "- this may mean you need to scale earlier, or use step scaling rather than target scaling\n",
    "- check the cpu, memory and gpu utilization metrics of your endpoint, likely 100%\n",
    "- the defaults used in this lab may show an example of failure, which after scaling occurs reduces\n",
    "  - however endpoints can take minutes to provision once a scale out alarm happens, once it does, those failures go away\n",
    "  - monitor the metrics to see the % increase, then decrease after scaling\n",
    "- failures can also be caused by api call throttling\n",
    "  - you can help this by increasing the retries maxattempts value in the locust script, or implementing exponential backoff as you would in your production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa72d8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# now we're going to use locust to simulate load on the endpoint\n",
    "# https://docs.locust.io/en/stable/ \n",
    "# https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/\n",
    "# see the locust_script_lab2.py file for details of the load test\n",
    "# it gathers the endpoint name, etc from the parameter store where we stored it in lab 1\n",
    "\n",
    "# Paste the following in a terminal window, make sure its run in your virtual environment created in lab 1, or in your own that has boto3 and locust installed\n",
    "# this will run in headless mode, use the properties just exported, and write csv logs for use in graph cells below\n",
    "locust -f locust_script_lab2.py --headless -u $LOCUST_USERS -r $LOCUST_SPAWN_RATE --run-time $LOCUST_RUN_TIME --host http://localhost --csv results --csv-full-history\n",
    "\n",
    "# for quick testing you can run with just 1 user, spawning at rate of 1 user per second, for 2 minutes\n",
    "locust -f locust_script_lab2.py --headless -u 1 -r 1 --run-time 2m --host http://localhost --csv results --csv-full-history\n",
    "\n",
    "# OR\n",
    "# if you want to see the locust web UI, then run without --headless and point your browser at http://localhost:8089\n",
    "# it will still write the csv files for use in graph cells below\n",
    "locust -f locust_script_lab2.py -u $LOCUST_USERS -r $LOCUST_SPAWN_RATE --run-time $LOCUST_RUN_TIME --host http://localhost --csv results --csv-full-history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e70e85",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Review Stats</span>\n",
    "- After Locust has finished, we can review the stats it wrote as csv files\n",
    "- NOTE the endpoint will not scale back in for 15 minutes (value may change depending on the scaling policy) after locust has finished its run - see above for a description of this\n",
    "  - therefore when viewing the charts below, it will not reflect a scale in event for 15 minutes \n",
    "\n",
    "#### SageMaker Endpoint Latency Under Load\n",
    "  - See how your SageMaker endpoint responded as load increased, and correlate that with autoscaling events\n",
    "  - NOTE this uses the results csv file written by locust, so wait until locust finishes before running this and subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88272da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot response times over time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the full history file\n",
    "df = pd.read_csv(\"results_stats_history.csv\")\n",
    "df.head()\n",
    "\n",
    "# Plot response times over time\n",
    "plt.plot(df[\"Timestamp\"], df[\"95%\"], label=\"p95 latency (ms)\")\n",
    "plt.plot(df[\"Timestamp\"], df[\"99%\"], label=\"p99 latency (ms)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.title(\"SageMaker Endpoint Latency Under Load\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22a6ff",
   "metadata": {},
   "source": [
    "#### Concurrent Users vs. Request Rate\n",
    "- Total RPS (Requests per second) → how busy your endpoint is at each interval\n",
    "- Concurrent Users → how many users were active at that moment\n",
    "- Compare these to your endpoint instance count and scaling policy in SageMaker to see if the scaling is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3db877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Concurrent Users and Requests per Second\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Left Y-axis: Requests per second\n",
    "ax1.plot(df[\"Timestamp\"], df[\"Requests/s\"], color=\"tab:blue\", label=\"Total RPS\")\n",
    "ax1.set_xlabel(\"Time (s)\")\n",
    "ax1.set_ylabel(\"Requests per second\", color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# Right Y-axis: Number of concurrent users\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df[\"Timestamp\"], df[\"User Count\"], color=\"tab:orange\", label=\"Concurrent Users\")\n",
    "ax2.set_ylabel(\"Concurrent Users\", color=\"tab:orange\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n",
    "\n",
    "# Titles & Legends\n",
    "plt.title(\"Concurrent Users vs. Request Rate\")\n",
    "fig.tight_layout()\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7d35e",
   "metadata": {},
   "source": [
    "#### Locust Failures\n",
    "- Any failures plotted against GPU metrics\n",
    "- Look in the results_failures.csv for details\n",
    "- If you see throttling errors, its because have exceeded the number of requests handled per second by your endpoint api\n",
    "  - In the locust script, you can increase retries, example below, or implement exponential backoff as you wold in your own production code\n",
    "    - region_name=region, retries={\"max_attempts\": 3, \"mode\": \"standard\"}\n",
    "- NOTE there is a lookback in minutes defined below, so the cell output is only relevant if its run shortly after locust has finished, any longer and it may not get results from the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66220ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "lookback_minutes = 60  # how far back to fetch metrics\n",
    "\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "aas = boto3.client(\"application-autoscaling\", region_name=region)\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(minutes=lookback_minutes)\n",
    "\n",
    "# --- 1. Get CloudWatch metric: ConcurrentRequestsPerModel ---\n",
    "metrics = cw.get_metric_statistics(\n",
    "    Namespace=\"/aws/sagemaker/Endpoints\",\n",
    "    MetricName=\"GPUUtilization\",\n",
    "    Dimensions=[{\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": variant_name}],\n",
    "    StartTime=start_time,\n",
    "    EndTime=end_time,\n",
    "    Period=60,\n",
    "    Statistics=[\"Average\"],\n",
    ")\n",
    "\n",
    "# Convert CloudWatch metric to DataFrame\n",
    "gpu_df = pd.DataFrame({\n",
    "    \"timestamp\": [dp['Timestamp'] for dp in metrics['Datapoints']],\n",
    "    \"GPU_Utilization\": [dp['Average'] for dp in metrics['Datapoints']]\n",
    "})\n",
    "gpu_df = gpu_df.sort_values(\"timestamp\")\n",
    "\n",
    "# --- Load Locust failures CSV ---\n",
    "locust_df = pd.read_csv(\"results_stats_history.csv\")\n",
    "# Convert Unix timestamp to datetime\n",
    "locust_df['timestamp'] = pd.to_datetime(locust_df['Timestamp'], unit='s', utc=True)\n",
    "\n",
    "# Use total failures per timestamp\n",
    "locust_df = locust_df[['timestamp', 'Total Failure Count']]\n",
    "\n",
    "# --- Merge on nearest timestamp ---\n",
    "merged = pd.merge_asof(\n",
    "    locust_df.sort_values('timestamp'),\n",
    "    gpu_df.sort_values('timestamp'),\n",
    "    on='timestamp'\n",
    ")\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax1.plot(merged['timestamp'], merged['GPU_Utilization'], 'b-', label='GPU Utilization (%)')\n",
    "ax1.set_ylabel('GPU Utilization (%)', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged['timestamp'], merged['Total Failure Count'], 'r-', label='Locust Fails')\n",
    "ax2.set_ylabel('Locust Fails', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "plt.title(f\"GPU vs Locust Failures for {endpoint_name}\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b9b05",
   "metadata": {},
   "source": [
    "#### SageMaker Endpoint Scaling\n",
    "- Concurrent Requests vs. Instance Count\n",
    "- Show when the instance scales\n",
    "- Pulls SageMaker endpoint metrics from CloudWatch\n",
    "- Handles empty or missing data gracefully\n",
    "- Converts them into a DataFrame for analysis\n",
    "- Plots the metric values over time\n",
    "- If the instance has not scaled in yet, its probably still within the 15 minutes (may be different depending on policy) of the scale in threshold, check the alarm in the console for details\n",
    "- NOTE there is a lookback in minutes defined below, so the cell output is only relevant if its run shortly after locust has finished, any longer and it may not get results from the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec17bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "lookback_minutes = 80  # how far back to fetch metrics\n",
    "\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "aas = boto3.client(\"application-autoscaling\", region_name=region)\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(minutes=lookback_minutes)\n",
    "\n",
    "# --- 1. Get CloudWatch metric: ConcurrentRequestsPerModel ---\n",
    "metrics = cw.get_metric_statistics(\n",
    "    Namespace=\"AWS/SageMaker\",\n",
    "    MetricName=\"ConcurrentRequestsPerModel\",\n",
    "    Dimensions=[{\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": variant_name}],\n",
    "    StartTime=start_time,\n",
    "    EndTime=end_time,\n",
    "    Period=60,\n",
    "    Statistics=[\"Average\"],\n",
    ")\n",
    "\n",
    "datapoints = metrics.get(\"Datapoints\", [])\n",
    "cw_df = pd.DataFrame(datapoints)\n",
    "\n",
    "if cw_df.empty:\n",
    "    print(\"⚠️ No CloudWatch datapoints found for ConcurrentRequestsPerModel\")\n",
    "else:\n",
    "    cw_df[\"Timestamp\"] = pd.to_datetime(cw_df[\"Timestamp\"])\n",
    "    cw_df.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "# --- 2. Get Current & Desired Instance Counts ---\n",
    "scalable_target = aas.describe_scalable_targets(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceIds=[f\"endpoint/{endpoint_name}/variant/{variant_name}\"],\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\"\n",
    ")\n",
    "desired_min = scalable_target[\"ScalableTargets\"][0][\"MinCapacity\"]\n",
    "desired_max = scalable_target[\"ScalableTargets\"][0][\"MaxCapacity\"]\n",
    "\n",
    "# --- 3. Get actual scaling activity history (DesiredInstanceCount changes) ---\n",
    "scaling_history = aas.describe_scaling_activities(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{endpoint_name}/variant/{variant_name}\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MaxResults=20\n",
    ")\n",
    "\n",
    "scaling_df = pd.DataFrame(scaling_history.get(\"ScalingActivities\", []))\n",
    "\n",
    "if not scaling_df.empty:\n",
    "    scaling_df[\"Timestamp\"] = pd.to_datetime(scaling_df[\"StartTime\"])\n",
    "    # Check if timestamps are timezone-aware\n",
    "    tzinfo = scaling_df[\"Timestamp\"].dt.tz  # will be None if naive\n",
    "    lookback_start = datetime.now(tz=tzinfo) - timedelta(minutes=lookback_minutes)\n",
    "\n",
    "    # Define cutoff\n",
    "    cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=lookback_minutes)\n",
    "    # Restrict to only rows newer than cutoff\n",
    "    scaling_df = scaling_df[scaling_df[\"Timestamp\"] >= cutoff_time].copy()\n",
    "    # extract the instance count\n",
    "    scaling_df[\"NewCapacity\"] = scaling_df[\"Description\"].str.extract(r\"(\\d+)(?=\\D*$)\").astype(float)\n",
    "    # Insert synthetic row at lookback start - this is get the graph to show 1 instance at the start of the lookback period\n",
    "    synthetic_row = pd.DataFrame({\n",
    "        \"Timestamp\": [lookback_start],\n",
    "        \"NewCapacity\": [1.0],  # default instance count\n",
    "        \"StatusCode\": [\"Simulated\"],\n",
    "    })\n",
    "    scaling_df = pd.concat([synthetic_row, scaling_df], ignore_index=True)\n",
    "\n",
    "    # Sort for plotting\n",
    "    scaling_df.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "# --- 4. Plot everything together ---\n",
    "if not cw_df.empty:\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot concurrency (left axis)\n",
    "    ax1.plot(cw_df[\"Timestamp\"], cw_df[\"Average\"], marker=\"o\", color=\"tab:blue\", label=\"ConcurrentRequestsPerModel\")\n",
    "    ax1.set_xlabel(\"Time\")\n",
    "    ax1.set_ylabel(\"Concurrent Requests\", color=\"tab:blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    # Plot desired instance count (right axis)\n",
    "    ax2 = ax1.twinx()\n",
    "    if not scaling_df.empty:\n",
    "        ax2.step(scaling_df[\"Timestamp\"], scaling_df[\"NewCapacity\"], where=\"post\", color=\"tab:red\", label=\"DesiredInstanceCount\")\n",
    "    ax2.set_ylabel(\"Instance Count\", color=\"tab:red\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "    ax2.set_ylim(0, desired_max + 1)\n",
    "\n",
    "    fig.suptitle(f\"SageMaker Endpoint Scaling: {endpoint_name}/{variant_name}\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(cw_df.head(10))\n",
    "    if not scaling_df.empty:\n",
    "        display(scaling_df[[\"Timestamp\", \"NewCapacity\", \"StatusCode\", \"StatusMessage\"]].head(10))\n",
    "        display(scaling_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f745c5",
   "metadata": {},
   "source": [
    "#### Concurrent Requests per Instance vs Locust Users\n",
    "- Calculates requests per instance over time\n",
    "- Plots Locust users vs requests per instance, marking the scaling threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "\n",
    "lookback_minutes = 120  # how far back to fetch metrics\n",
    "\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "aas = boto3.client(\"application-autoscaling\", region_name=region)\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(minutes=lookback_minutes)\n",
    "\n",
    "# --- 1. Get CloudWatch metric: ConcurrentRequestsPerModel ---\n",
    "metrics = cw.get_metric_statistics(\n",
    "    Namespace=\"AWS/SageMaker\",\n",
    "    MetricName=\"ConcurrentRequestsPerModel\",\n",
    "    Dimensions=[{\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": variant_name}],\n",
    "    StartTime=start_time,\n",
    "    EndTime=end_time,\n",
    "    Period=60,\n",
    "    Statistics=[\"Average\"],\n",
    ")\n",
    "\n",
    "datapoints = metrics.get(\"Datapoints\", [])\n",
    "cw_df = pd.DataFrame(datapoints)\n",
    "\n",
    "if cw_df.empty:\n",
    "    print(\"⚠️ No CloudWatch datapoints found for ConcurrentRequestsPerModel\")\n",
    "else:\n",
    "    cw_df[\"Timestamp\"] = pd.to_datetime(cw_df[\"Timestamp\"])\n",
    "    cw_df.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "# --- 2. Get Current & Desired Instance Counts ---\n",
    "scalable_target = aas.describe_scalable_targets(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceIds=[f\"endpoint/{endpoint_name}/variant/{variant_name}\"],\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\"\n",
    ")\n",
    "desired_min = scalable_target[\"ScalableTargets\"][0][\"MinCapacity\"]\n",
    "desired_max = scalable_target[\"ScalableTargets\"][0][\"MaxCapacity\"]\n",
    "\n",
    "# --- 3. Get actual scaling activity history (DesiredInstanceCount changes) ---\n",
    "scaling_history = aas.describe_scaling_activities(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{endpoint_name}/variant/{variant_name}\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MaxResults=20\n",
    ")\n",
    "\n",
    "scaling_df = pd.DataFrame(scaling_history.get(\"ScalingActivities\", []))\n",
    "\n",
    "if not scaling_df.empty:\n",
    "    scaling_df[\"Timestamp\"] = pd.to_datetime(scaling_df[\"StartTime\"])\n",
    "    # Check if timestamps are timezone-aware\n",
    "    tzinfo = scaling_df[\"Timestamp\"].dt.tz  # will be None if naive\n",
    "    lookback_start = datetime.now(tz=tzinfo) - timedelta(minutes=lookback_minutes)\n",
    "\n",
    "    # Define cutoff\n",
    "    cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=lookback_minutes)\n",
    "    # Restrict to only rows newer than cutoff\n",
    "    scaling_df = scaling_df[scaling_df[\"Timestamp\"] >= cutoff_time].copy()\n",
    "    # extract the instance count\n",
    "    scaling_df[\"NewCapacity\"] = scaling_df[\"Description\"].str.extract(r\"(\\d+)(?=\\D*$)\").astype(float)\n",
    "    # Insert synthetic row at lookback start - this is get the graph to show 1 instance at the start of the lookback period\n",
    "    synthetic_row = pd.DataFrame({\n",
    "        \"Timestamp\": [lookback_start],\n",
    "        \"NewCapacity\": [1.0],  # default instance count\n",
    "        \"StatusCode\": [\"Simulated\"],\n",
    "    })\n",
    "    scaling_df = pd.concat([synthetic_row, scaling_df], ignore_index=True)\n",
    "\n",
    "    # Sort for plotting\n",
    "    scaling_df.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "# --- 4. Calculate Requests Per Instance ---\n",
    "if not cw_df.empty and not scaling_df.empty:\n",
    "    # Create a DataFrame with instance counts at each minute\n",
    "    instance_counts = pd.DataFrame()\n",
    "    instance_counts[\"Timestamp\"] = cw_df[\"Timestamp\"]\n",
    "    \n",
    "    # For each timestamp in cw_df, find the most recent instance count from scaling_df\n",
    "    instance_counts[\"InstanceCount\"] = instance_counts[\"Timestamp\"].apply(\n",
    "        lambda ts: scaling_df[scaling_df[\"Timestamp\"] <= ts][\"NewCapacity\"].iloc[-1] \n",
    "        if not scaling_df[scaling_df[\"Timestamp\"] <= ts].empty else 1.0\n",
    "    )\n",
    "    \n",
    "    # Calculate requests per instance\n",
    "    cw_df[\"RequestsPerInstance\"] = cw_df[\"Average\"] / instance_counts[\"InstanceCount\"]\n",
    "    \n",
    "    # Also add instance count to cw_df for reference\n",
    "    cw_df[\"InstanceCount\"] = instance_counts[\"InstanceCount\"]\n",
    "\n",
    "# --- 5. Plot everything together with aligned x-axis ---\n",
    "if not cw_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)  # Added sharex=True\n",
    "    \n",
    "    # Get common x-axis limits\n",
    "    x_min = cw_df[\"Timestamp\"].min()\n",
    "    x_max = cw_df[\"Timestamp\"].max()\n",
    "    \n",
    "    # Plot 1: Total concurrent requests and instance count\n",
    "    ax1.plot(cw_df[\"Timestamp\"], cw_df[\"Average\"], marker=\"o\", color=\"tab:blue\", \n",
    "             label=\"Total Concurrent Requests\", markersize=4)\n",
    "    ax1.set_ylabel(\"Total Concurrent Requests\", color=\"tab:blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add instance count to first plot\n",
    "    ax1_inst = ax1.twinx()\n",
    "    if not scaling_df.empty:\n",
    "        ax1_inst.step(scaling_df[\"Timestamp\"], scaling_df[\"NewCapacity\"], where=\"post\", \n",
    "                     color=\"tab:red\", linewidth=2, label=\"Instance Count\")\n",
    "    ax1_inst.set_ylabel(\"Instance Count\", color=\"tab:red\")\n",
    "    ax1_inst.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "    ax1_inst.set_ylim(0, desired_max + 1)\n",
    "    \n",
    "    # Combine legends for first plot\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax1_inst.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    # Plot 2: Requests per instance\n",
    "    if \"RequestsPerInstance\" in cw_df.columns:\n",
    "        ax2.plot(cw_df[\"Timestamp\"], cw_df[\"RequestsPerInstance\"], marker=\"s\", \n",
    "                color=\"tab:green\", label=\"Requests Per Instance\", linewidth=2, markersize=4)\n",
    "        ax2.axhline(y=desired_max, color=\"red\", linestyle=\"--\", alpha=0.7, \n",
    "                   label=f\"Scale-out threshold (est. ~{desired_max})\")\n",
    "        ax2.axhline(y=desired_min, color=\"orange\", linestyle=\"--\", alpha=0.7, \n",
    "                   label=f\"Scale-in threshold (est. ~{desired_min})\")\n",
    "        ax2.set_ylabel(\"Requests Per Instance\", color=\"tab:green\")\n",
    "        ax2.set_xlabel(\"Time\")  # Only bottom plot gets x-label\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Add instance count as text annotations for scaling events\n",
    "        if not scaling_df.empty:\n",
    "            for _, event in scaling_df[scaling_df[\"StatusCode\"] != \"Simulated\"].iterrows():\n",
    "                ax2.axvline(x=event[\"Timestamp\"], color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "                ax2.text(event[\"Timestamp\"], ax2.get_ylim()[1] * 0.9, \n",
    "                        f\"Inst: {int(event['NewCapacity'])}\", \n",
    "                        rotation=90, va='top', ha='right', fontsize=8)\n",
    "\n",
    "    # Set common x-axis limits\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "    \n",
    "    # Format x-axis ticks to be more readable\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.suptitle(f\"SageMaker Endpoint Scaling: {endpoint_name}/{variant_name}\\n\"\n",
    "                f\"Min: {desired_min}, Max: {desired_max} instances\", fontsize=14)\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for suptitle\n",
    "    plt.show()\n",
    "\n",
    "    # Display the data\n",
    "    print(\"📊 CloudWatch Metrics (with per-instance calculations):\")\n",
    "    display_cols = [\"Timestamp\", \"Average\", \"InstanceCount\", \"RequestsPerInstance\"] if \"RequestsPerInstance\" in cw_df.columns else [\"Timestamp\", \"Average\"]\n",
    "    display(cw_df[display_cols].head(10))\n",
    "    \n",
    "    if not scaling_df.empty:\n",
    "        print(\"\\n⚡ Scaling Activities:\")\n",
    "        display(scaling_df[[\"Timestamp\", \"NewCapacity\", \"StatusCode\", \"StatusMessage\"]].head(10))\n",
    "        \n",
    "        # Show summary statistics\n",
    "        if \"RequestsPerInstance\" in cw_df.columns:\n",
    "            print(f\"\\n📈 Requests Per Instance Statistics:\")\n",
    "            print(f\"   Average: {cw_df['RequestsPerInstance'].mean():.2f}\")\n",
    "            print(f\"   Maximum: {cw_df['RequestsPerInstance'].max():.2f}\")\n",
    "            print(f\"   Minimum: {cw_df['RequestsPerInstance'].min():.2f}\")\n",
    "            print(f\"   Std Dev: {cw_df['RequestsPerInstance'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3d83b",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">SageMaker Inference Recommender</span>\n",
    "- Helps you select the best instance type and configuration for your ML models and workloads\n",
    "- Suitable for traditional ML and deep learning models - **no documented usage for LLMs** - added here for reference only\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html\n",
    "- NOTE NEEDS TO BE INVESTIGATED TO SEE IF IT WORKS WITH LLM REAL TIME ENDPOINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfc496",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Clean Up Architecture</span>\n",
    "### <span style=\"color:Red\">Only do this if you have finished with this lab and any labs that depend on it!</span>\n",
    "##### It will delete all architecture created, make sure you no longer need any of it!!!\n",
    "\n",
    "### <span style=\"color:Red\">If you are moving onto lab 3 to look at step scaling, you have choices!</span>\n",
    "##### Delete and deregister the target tracking policy used in this lab\n",
    "##### OR\n",
    "##### Move directly to lab 3, but note the endpoint will then use both a target tracking policy and the step policy created in lab 3\n",
    "##### Having both these types of policies can cause conflicts, so it is best to delete and deregister the target tracking policy via the 2 cells below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e740a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the auto scaling policy and scalable target if you want to clean up\n",
    "# first delete the scaling policy\n",
    "response = autoScaling.delete_scaling_policy(\n",
    "    PolicyName=myEndpointScalingPolicyName,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=ResourceId,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\"\n",
    ")\n",
    "print(response)\n",
    "print ('Done! Move to the next cell ->')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then deregister the scalable target\n",
    "response = autoScaling.deregister_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=ResourceId,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\"\n",
    ")\n",
    "print(response)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint and all other architecture created in lab 1 can be deleted in lab 1 notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
