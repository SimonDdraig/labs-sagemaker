{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06920b65",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:DarkSeaGreen\">JumpStart Lab 1</span>\n",
    "\n",
    "This lab does the following:\n",
    "\n",
    "- Provision a model via Jumpstart\n",
    "- Create a JumpStart endpoint\n",
    "- Interacts with the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc4b84",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">requirements_lab1.txt</span>\n",
    "- Most of the requirements just get the latest version\n",
    "- However, on Nov 19 2025 AWS released SageMaker 3.0.1 SDK, this is compatable up to Python 3.12\n",
    "  - At time of writing does not yet support Python 3.13+, (latest current version is Python 3.14)\n",
    "- SageMaker 3.0.1 is forced in the requirements file, otherwise it only gets 3.0.0 which fails due to dependency issues as its sub files are not included, AWS fixed this is 3.0.1 which was released immediately after 3.0.0 :)\n",
    "- Therefore make sure your Python venv is created using 3.12 only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb29b8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Prepare Your Environment</span>\n",
    "### Requirements for this Jupyter Notebook Lab if running in VSCode or equivalent local IDE\n",
    "##### Note these are macOS specific\n",
    "- Credentials\n",
    "  - You need credentials to your AWS account to execute this Jupyter Lab if running locally from your laptop\n",
    "    - Locally: Credentials and therefore permissions asscociated with the IAM user (with CLI access enabled) are provided by AWS configure connection to your AWS account\n",
    "    - Cloud: Permissions provided via logged in user\n",
    "- Installers:\n",
    "  - Pip\n",
    "    - Python libraries\n",
    "    - Works inside Python envs\n",
    "  - homebrew (brew) (mac)\n",
    "    - System software, tools, and dependencies\n",
    "    - Works at OS level\n",
    "\n",
    "- Run the commands of the cell below in a terminal window to create a virtual environment if you need one\n",
    "  - Note check your Python version first, then if ok, copy the rest and run in terminal window\n",
    "  - Note if you copy and paste the multiple lines and run as one you will get zsh: command not found: # errors because of the comments, but you can ignore\n",
    "  - Remember to restart the kernel to pick up the new venv\n",
    "  - The venv can be deleted via the last cell in this notebook if no longer needed\n",
    "- If you already have a virtual environment, then just activate it as shown in the second cell below\n",
    "  - Venv (can be created below) used by this notebook is *venv-jumpstart-stable-lab1*\n",
    "\n",
    "#### SageMaker can release breaking changes to this code, see link for details if any cell fails with a SageMaker issue\n",
    "- This Jupyter Notebook was written with SageMaker V3.0.1 released 20 Nov 2025\n",
    "- At time of creating this notebook (Nov 26), SageMaker 3.0.1 only supports Python 3.12\n",
    "  - https://github.com/aws/sagemaker-python-sdk/blob/master/CHANGELOG.md\n",
    "  - https://github.com/aws/sagemaker-python-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e8873",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check your credentials (AWS identity) to confirm you are using the right credentials\n",
    "# run in a terminal window \n",
    "aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27832229",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "### STOP ###\n",
    "### IF USING THIS NOTEBOOK IN A SAGEMAKER JUPYTER NOTEBOOK INSTANCE, THEN SKIP TO THE NEXT CELL ###\n",
    "### OTHERWISE, IF USING VSCODE OR EQUIVALENT LOCAL IDE, THEN CONTINUE BELOW ###\n",
    "### This script is for setting up your environment for the JumpStart Lab 1 ###\n",
    "# do you need to upgrade python first? Your available version of Python is used to create the virtual environment\n",
    "python3 --version\n",
    "\n",
    "### STOP ###\n",
    "### DO YOU NEED TO UPGRADE PYTHON ###\n",
    "# upgrade to the latest version of python if required\n",
    "brew install python==3.12\n",
    "# restart vscode to pickup new version of python\n",
    "python3 --version\n",
    "\n",
    "### STOP ###\n",
    "### OK IF YOU HAVE THE CORRECT VERSION OF PYTHON, CONTINUE ###\n",
    "# create in the folder of this notebook, eg Documents/github/labs-sagemaker/jumpstart/image_upscale\n",
    "# create a virtual environment\n",
    "python3.12 -m venv venv-jumpstart-stable-lab1\n",
    "# activate the virtual environment\n",
    "source venv-jumpstart-stable-lab1/bin/activate\n",
    "### COPY TO HERE ONLY IF RUNNING AS ONE COPY AND PASTE ###\n",
    "\n",
    "### STOP ###\n",
    "### MAKE SURE ABOVE VENV GETS ACTIVATED BEFORE RUNNING THE REST ###\n",
    "# upgrade pip\n",
    "pip install --upgrade pip\n",
    "# jupyter kernel support\n",
    "pip install ipykernel\n",
    "# add the virtual environment to jupyter\n",
    "python  -m ipykernel install --user --name=venv-jumpstart-stable-lab1 --display-name \"Python (venv-jumpstart-stable-lab1)\"\n",
    "# install the required packages - may need to specify the path here if not in the correct folder in terminal window\n",
    "pip install -r requirements_lab1.txt\n",
    "# pip install -r Documents/github/labs-sagemaker/jumpstart/etc/requirements_lab1.txt\n",
    "# verify the installation\n",
    "pip list\n",
    "\n",
    "### RESTART VSCODE TO PICKUP THE NEW VENV ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcb8f1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "### STOP ###\n",
    "### This command is for activating an environment that already exists, its for use in a terminal window if you need it ###\n",
    "source venv-jumpstart-stable-lab1/bin/activate\n",
    "pip list\n",
    "\n",
    "# use pip freeze if you prefer for requirements.txt freiendly format\n",
    "### ALSO MAKE SURE YOU SELECT IT AS YOUR KERNEL FOR THIS JUPYTER NOTEBOOK ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb8cd5",
   "metadata": {},
   "source": [
    "# Lab 1 Starts Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86ebb0",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Setup</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region\n",
    "# for the purpose of this lab, us-east-1, us-west-2, eu-west-1 has the broadest coverage of JumpStart models and instance types\n",
    "# if you provision in other regions, you may not have access to all the models or instance types, \n",
    "# and may need to request increase of quotas for endpoint usage for some instance types\n",
    "myRegion='us-east-1'\n",
    "\n",
    "# iam\n",
    "myRoleSageMakerExecution=\"doit-jumpstart-sagemaker-execution-role\"\n",
    "myRoleSageMakerExecutionARN='RETRIEVED FROM ROLE BELOW'\n",
    "\n",
    "# parameter store\n",
    "myParameterStoreChosenModel='doit-jumpstart-sagemaker-chosen-model'\n",
    "myParameterStoreEndpointName='doit-jumpstart-sagemaker-endpoint-name'\n",
    "myParameterStoreIAMARN='doit-jumpstart-sagemaker-iam-arn'\n",
    "\n",
    "# endpoint\n",
    "myEndpoint='doit-jumpstart-endpoint'\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95075000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from certifi import where\n",
    "\n",
    "botoSession = boto3.Session(region_name=myRegion)\n",
    "\n",
    "# Configure boto3 to use certifi's certificates - helps avoid SSL errors if your system’s certificate store is out of date or missing root certs\n",
    "sts_client = boto3.client('sts', verify=where())\n",
    "myAccountNumber = sts_client.get_caller_identity()[\"Account\"]\n",
    "print(myAccountNumber)\n",
    "print(sts_client.get_caller_identity()[\"Arn\"])\n",
    "\n",
    "# create clients we can use later\n",
    "# iam\n",
    "iam = boto3.client('iam', region_name=myRegion, verify=where())\n",
    "# ssm\n",
    "ssm = boto3.client('ssm', region_name=myRegion, verify=where())\n",
    "# sagemaker\n",
    "sm = boto3.client(\"sagemaker\", region_name=myRegion, verify=where())\n",
    "smr = boto3.client(\"sagemaker-runtime\", region_name=myRegion, verify=where())\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tags added to all services we create\n",
    "myTags = [\n",
    "    {\"Key\": \"env\", \"Value\": \"non_prod\"},\n",
    "    {\"Key\": \"owner\", \"Value\": \"doit-jumpstart\"},\n",
    "    {\"Key\": \"project\", \"Value\": \"lab1\"},\n",
    "    {\"Key\": \"author\", \"Value\": \"simon\"},\n",
    "]\n",
    "myTagsDct = {\n",
    "    \"env\": \"non_prod\",\n",
    "    \"owner\": \"doit-jumpstart\",\n",
    "    \"project\": \"lab1\",\n",
    "    \"author\": \"simon\",\n",
    "}\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e5c531",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get SageMaker Execution Role</span>\n",
    "- We need to get the execution role SageMaker uses to execute its commands.\n",
    "- We do this differently if \n",
    "  - running in a SageMaker Jupyter notebook\n",
    "    - OR\n",
    "  - running in a local IDE\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-role.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67085b",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">IAM</span>\n",
    "- The following method is only called if this is being run in a local IDE\n",
    "- It will an IAM execution role used by SageMaker\n",
    "- The following cell will NOT create the role, it will only create the role if the method it defines is called below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSageMakerExecutionRole():\n",
    "    \"\"\"\n",
    "    Creates a role required for SageMaker to run jobs on your behalf\n",
    "    Only needed if this is being run in a local IDE, not needed if in SageMaker Studio or SageMaker Notebook Instance\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        An IAM execution role ARN\n",
    "    \"\"\"\n",
    "\n",
    "    # trust policy for the role\n",
    "    roleTrust = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"sagemaker.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # check if the role exists\n",
    "    try:\n",
    "        role = iam.get_role(RoleName=myRoleSageMakerExecution)\n",
    "        print(\"Role already exists. Using the existing role.\")\n",
    "        return role['Role']['Arn']\n",
    "    except iam.exceptions.NoSuchEntityException:\n",
    "        print(\"Role does not exist. Creating a new role.\")\n",
    "        \n",
    "    # create execution role for sagemaker - allows SageMaker notebook instances, training jobs, and models to access S3, ECR, and CloudWatch on your behalf\n",
    "    # this role is only created if we are running this notebook in a local ide, if we are in a jupyterlab in sagemaker studio, we dont need it as already created and available\n",
    "    role = iam.create_role(\n",
    "        RoleName=myRoleSageMakerExecution,\n",
    "        AssumeRolePolicyDocument=json.dumps(roleTrust),\n",
    "        Description=\"Service excution role for sagemaker ai use including inside jupyter notebooks\",\n",
    "        Tags=[\n",
    "            *myTags,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # attach managed policy to the role AmazonSageMakerFullAccess\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=myRoleSageMakerExecution,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n",
    "    )\n",
    "\n",
    "    # store the role arn in parameter store for use in other notebooks\n",
    "    ssm.put_parameter(\n",
    "        Name=myParameterStoreIAMARN,\n",
    "        Description='The ARN of the IAM role used by SageMaker for execution of jobs',\n",
    "        Value=role['Role']['Arn'],\n",
    "        Type='String',\n",
    "        Tags=[\n",
    "            *myTags,\n",
    "        ],\n",
    "    )   \n",
    "\n",
    "    return role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708857ff",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get Execution Role and Session</span>\n",
    "- SageMaker requires an execution role to assume on your behalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e08fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "sagemaker_session = Session()\n",
    "\n",
    "try:\n",
    "    # if this is being run in a SageMaker AI JupyterLab Notebook\n",
    "    myRoleSageMakerExecutionARN = get_execution_role()\n",
    "except:\n",
    "    # if this is being run in a local IDE - we need to create our own role\n",
    "    myRoleSageMakerExecutionARN = getSageMakerExecutionRole()\n",
    "\n",
    "# make sure we get a session in the correct region (needed as it can use the aws configure region if running this locally\n",
    "sageMakerSession = Session(boto_session=botoSession)\n",
    "\n",
    "print(myRoleSageMakerExecutionARN)\n",
    "print(sageMakerSession)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56ace1",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Provision a JumpStart Model</span>\n",
    "- Provision a model via Jumpstart\n",
    "- If you prefer, you can also do this via the JumpStart console, but you will have to bring in the endpoint that you create to continue with this code\n",
    "### Example models to provision\n",
    "- Stable Diffusion x4 upscaler FP16\n",
    "  - https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler/blame/fp16/README.md\n",
    "  - *model_id, model_version = \"model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16\", \"*\"*\n",
    "  - upscaling with Stable Diffusion (x4) is computationally expensive\n",
    "    - FP16 means it uses half-precision floating point, so you want a GPU with good Tensor Core\n",
    "  - the x4 upscaler model itself is large\n",
    "    - want ≥ 16 GB VRAM to run comfortably in FP16 for 512×512 → 2048×2048 upscales\n",
    "    - p4d.24xlarge (enterprise-grade, overkill unless you’re batching lots of requests)\n",
    "      - **needs an aws quota increase for this instance for endpoint usage**\n",
    "    - ml.g5.4xlarge\n",
    "      - good for a poc - widely supported, good memory, reasonably costed\n",
    "      - anything smaller and you will likely get CUDA out of memory errors\n",
    "        - you need plenty of GPU memory\n",
    "      - **needs an aws quota increase for this instance for endpoint usage**\n",
    "- see https://aws.amazon.com/sagemaker/ai/pricing/ for pricing, **larger instances can be very expensive per hour**\n",
    "- If you deply the model and you get a quota error, you will need to visit Service Quotas via the console and request an increase\n",
    "  - go to SageMaker service and search for the instance\n",
    "  - select the *model* for endpoint usage\n",
    "  - make sure your quota allows for auto scaling max\n",
    "- DO NOT LEAVE LARGE INSTANCES RUNNING LONGER THAN YOU NEED TO $$$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768bfb0",
   "metadata": {},
   "source": [
    "### Instance Size is Important\n",
    "- We are usinbg a model that upscales\n",
    "- The larger the original image, the more GPU memory is taken when upscaling\n",
    "- Sagemaker typically uses one GPU to do this \n",
    "  - SageMaker model endpoints don’t automatically spread inference across multiple GPUs unless the container is written for it\n",
    "- Stability Diffusion provides a diffuser library \n",
    "  - Breaks the image into smaller patches, processes sequentially, then stitches\n",
    "  - Uses much less VRAM at the cost of a bit more time\n",
    "  - We don't use that in this lab\n",
    "- p4d.24xlarge has more GPU memory, but maybe an overkill, expensive and won't scale if source images are still too large to upscale in one GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model we want to provision - THIS DISPLAYS AN INPUT BOX FOR YOU TO CHOOSE A MODEL\n",
    "# jump into the console, click on JumpStart, find a model you like and copy the model id from the details page\n",
    "# https://aws.amazon.com/sagemaker/ai/pricing/\n",
    "options = [\n",
    "    \"img2img|model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16|ml.p4de.24xlarge $$$$$\",\n",
    "    \"img2img|model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16|ml.p4d.24xlarge $$$$\",\n",
    "    \"img2img|model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16|ml.g5.48xlarge $$$$$\",\n",
    "    \"img2img|model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16|ml.g5.24xlarge $$$$\",\n",
    "    \"img2img|model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16|ml.g5.12xlarge $$$\",\n",
    "    \"img2img|model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16|ml.g5.2xlarge $$\",\n",
    "]\n",
    "\n",
    "print(\"Select an option:\")\n",
    "for i, opt in enumerate(options, 1):\n",
    "    print(f\"{i}. {opt}\")\n",
    "\n",
    "choice = int(input(\"Enter the number of the spec you want: \"))\n",
    "selected = options[choice - 1]\n",
    "\n",
    "modelType = selected.split(\"|\")[0]\n",
    "modelID = selected.split(\"|\")[1]\n",
    "instanceType = selected.split(\"|\")[2].split(\" \")[0]\n",
    "print(f\"You selected: model type {modelType} {modelID} on {instanceType}\")\n",
    "\n",
    "# store the model in a parameter store for use in other labs\n",
    "ssm.put_parameter(\n",
    "    Name=myParameterStoreChosenModel,\n",
    "    Description='the model chosen in lab1',\n",
    "    Value=selected,\n",
    "    Type='String',\n",
    "    Overwrite=True,\n",
    ")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebc21a",
   "metadata": {},
   "source": [
    "### Previous Errors using JumpStart Container\n",
    "- JumpStart containers load the entire model onto a single GPU\n",
    "- does not split the model across GPUs\n",
    "- does not automatically use multiple GPUs\n",
    "\n",
    "ml.g5.12xlarge:\n",
    "\n",
    "ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) \n",
    "from primary with message \"{\n",
    "  \"code\": 400,\n",
    "  \"type\": \"InternalServerException\",\n",
    "  \"message\": \"CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 22.20 GiB total capacity; 17.79 GiB already \n",
    "allocated; 2.57 GiB free; 17.81 GiB reserved in total by PyTorch) If reserved memory is \\u003e\\u003e allocated \n",
    "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
    "PYTORCH_CUDA_ALLOC_CONF\"\n",
    "}\n",
    "\n",
    "ml.g5.24xlarge:\n",
    "ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) \n",
    "from primary with message \"{\n",
    "  \"code\": 400,\n",
    "  \"type\": \"InternalServerException\",\n",
    "  \"message\": \"CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 22.20 GiB total capacity; 17.79 GiB already \n",
    "allocated; 2.59 GiB free; 17.79 GiB reserved in total by PyTorch) If reserved memory is \\u003e\\u003e allocated \n",
    "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
    "PYTORCH_CUDA_ALLOC_CONF\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dffb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ModelBuilder from JumpStart Config using the model and instance you selected in the previous cell\n",
    "# this will take a while (few seconds), as it needs to download the model from jumpstart\n",
    "\n",
    "from sagemaker.serve.model_builder import ModelBuilder\n",
    "from sagemaker.core.jumpstart.configs import JumpStartConfig\n",
    "from sagemaker.core.resources import EndpointConfig\n",
    "from sagemaker.train.configs import Compute\n",
    "\n",
    "# https://github.com/aws/sagemaker-python-sdk/blob/master/v3-examples/inference-examples/jumpstart-example.ipynb\n",
    "jumpstart_config = JumpStartConfig(model_id=modelID)\n",
    "compute = Compute(instance_type=instanceType)\n",
    "model_builder = ModelBuilder.from_jumpstart_config(\n",
    "    jumpstart_config=jumpstart_config,\n",
    "    compute=compute,\n",
    "    role_arn=myRoleSageMakerExecutionARN,\n",
    "    sagemaker_session=sageMakerSession\n",
    ")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "core_model = model_builder.build(\n",
    "    model_name=modelID,\n",
    "    role_arn=myRoleSageMakerExecutionARN,\n",
    "    sagemaker_session=sageMakerSession,\n",
    ")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the model to an endpoint\n",
    "core_endpoint = model_builder.deploy(\n",
    "    endpoint_name=myEndpoint,\n",
    "    role_arn=myRoleSageMakerExecutionARN,\n",
    "    sagemaker_session=sageMakerSession,\n",
    "    container_timeout_in_seconds=600,\n",
    ")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predictor name in a parameter store for use in other notebooks\n",
    "ssm.put_parameter(\n",
    "    Name=myParameterStoreEndpointName,\n",
    "    Description='the name of the sagemaker endpoint created in lab1',\n",
    "    Value=core_endpoint.endpoint_name,\n",
    "    Type='String',\n",
    "    Overwrite=True,\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965df58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays an image from the model response so it can be reviewed live rather thna diving into S3\n",
    "def decode_and_show(description, model_response) -> None:\n",
    "    from PIL import Image\n",
    "    import base64\n",
    "    import io\n",
    "    \n",
    "    print (description)\n",
    "    # Handle PIL Image objects\n",
    "    if hasattr(model_response, 'save'):  # Check if it's a PIL Image\n",
    "        display(model_response)\n",
    "        return\n",
    "    \n",
    "    # Handle bytes (raw image data)\n",
    "    elif isinstance(model_response, bytes):\n",
    "        image = Image.open(io.BytesIO(model_response))\n",
    "        display(image)\n",
    "        image.close()\n",
    "    \n",
    "    # Handle base64 string (encoded image)\n",
    "    elif isinstance(model_response, str):\n",
    "        image = Image.open(io.BytesIO(base64.b64decode(model_response)))\n",
    "        display(image)\n",
    "        image.close()\n",
    "    \n",
    "    # Handle list of base64 strings (model response)\n",
    "    elif isinstance(model_response, list):\n",
    "        for i, img_data in enumerate(model_response):\n",
    "            image = Image.open(io.BytesIO(base64.b64decode(img_data)))\n",
    "            print(f\"Image {i + 1}:\")\n",
    "            display(image)\n",
    "            image.close()\n",
    "    \n",
    "    else:\n",
    "        print(f\"Can't handle the image. Unexpected response type: {type(model_response)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the image if required - large images can cause cuda memory errors dues to memory reqd to scale it\n",
    "def resize_image(imgBytes, max_size=1024):\n",
    "    from PIL import Image\n",
    "    import io\n",
    "\n",
    "    # Resize maintaining aspect ratio\n",
    "    image = Image.open(io.BytesIO(imgBytes))\n",
    "    decode_and_show(\"Original Image\", image)\n",
    "    \n",
    "    image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "    decode_and_show(\"Downsized Image\", image)\n",
    "\n",
    "    # Convert back to bytes\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format='JPEG', quality=95)  # Use JPEG to reduce size\n",
    "    return img_byte_arr.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the endpoint with some example payloads\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "#Make sure your jpeg images are < 6MB otherwise you may get a HTTP 413 Content Too Large (Payload Too Large) error\n",
    "# try your own payload\n",
    "images = [\n",
    "    \"resources/img2_original_1024.jpeg\",\n",
    "    \"resources/img1_original_1024.jpeg\",\n",
    "]\n",
    "\n",
    "# Note that sending or receiving the payload with the raw RGB values may hit default limits for the input payload and the response size\n",
    "# Therefore, we recommend using the base64 encoded image by setting:\n",
    "# content_type = “application/json;jpeg” and accept = “application/json;jpeg”\n",
    "# https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/\n",
    "content_type = \"application/json;jpeg\"\n",
    "accept = \"application/json;jpeg\"\n",
    "\n",
    "for img in images:\n",
    "    with open(img, \"rb\") as f:\n",
    "        bytes = f.read()\n",
    "    #downsized_bytes = resize_image(bytes, 128)\n",
    "    encoded_image = base64.b64encode(bytearray(bytes)).decode()\n",
    "\n",
    "    # NOTE \n",
    "    # num_inference_steps\n",
    "    # 10-25 steps: Fast, decent quality (good for testing/quick iterations)\n",
    "    # 25-50 steps: Good balance of speed and quality (common for production)\n",
    "    # 50-100 steps: High quality, slower (for final outputs)\n",
    "    # 100+ steps: Diminishing returns, much slower\n",
    "    #\n",
    "    # guidance_scale explanation\n",
    "    # Lower values (1-5): More creative, less faithful to prompt\n",
    "    # Medium values (7-12): Good balance (7.5 is a common default)\n",
    "    # Higher values (13-20+): Strictly follows prompt, less creative\n",
    "    payload = {\n",
    "        \"image\": encoded_image,\n",
    "        \"prompt\": \"Improve the photographic quality of the image\",\n",
    "        \"num_inference_steps\":25,\n",
    "        \"guidance_scale\":7.5\n",
    "    }\n",
    "\n",
    "    # NOTE if you get a 413 error, your original image sizes are probably too large, try images < 6MB\n",
    "    # HTTP 413 Content Too Large (Payload Too Large)\n",
    "    # NOTE if you get a 400 CUDA out of memory error, this indicates that your GPU RAM (Random access memory) is full \n",
    "    # HTTP 400 InternalServerException\n",
    "    # If so, bigger instance with a larger GPU per core, use a diffuser such as below to split the image into batches and restitch once done\n",
    "    # Or resize your images down to a smaller size\n",
    "    # NOTE if you get a timeout error, could be your image is too large\n",
    "    # If so, resize your images down to a smaller size\n",
    "    # Or reduce the num_inference_steps\n",
    "\n",
    "    # The following commented code does not work\n",
    "    # Seems SageMaker V3 does not yet support the core_endpoint.invoke method as documented\n",
    "    # https://github.com/aws/sagemaker-python-sdk/blob/master/v3-examples/inference-examples/jumpstart-example.ipynb\n",
    "    # possibly because of a missing __dict__ attribute on the payload object as it has a bytes body\n",
    "    # TypeError: vars() argument must have __dict__ attribute\n",
    "    #response = core_endpoint.invoke(\n",
    "    #    body = json.dumps(payload).encode('utf-8'),\n",
    "    #    content_type = content_type,\n",
    "    #    accept = accept,\n",
    "    #)\n",
    "\n",
    "    response = smr.invoke_endpoint(\n",
    "        EndpointName=core_endpoint.endpoint_name,\n",
    "        Body=json.dumps(payload).encode('utf-8'),\n",
    "        ContentType=content_type,\n",
    "        Accept=accept,\n",
    "    )\n",
    "\n",
    "    decode_and_show(\"Scaled 4x Image\", response[\"generated_images\"])\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfc496",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Move to Lab 2</span>\n",
    "# <span style=\"color:DarkSeaGreen\">OR...</span>\n",
    "# <span style=\"color:DarkSeaGreen\">Clean Up Architecture</span>\n",
    "### <span style=\"color:Red\">Only do this if you have finished with this lab and any labs that depend on it!</span>\n",
    "##### It will delete all architecture created, make sure you no longer need any of it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when finished with the endpoint, delete it\n",
    "response = sm.describe_endpoint(EndpointName=myEndpoint)\n",
    "print(response[\"EndpointStatus\"])\n",
    "\n",
    "if response[\"EndpointStatus\"] == \"InService\":\n",
    "    print(\"Endpoint is in service. Proceeding with deletion.\")\n",
    "    sm.delete_endpoint(EndpointName=myEndpoint)\n",
    "    print ('Done! Move to the next cell ->')\n",
    "else:\n",
    "    print(\"Endpoint is not in service. Cannot delete. Try again in a couple of minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the endpoint config\n",
    "response = sm.delete_endpoint_config(EndpointConfigName=myEndpoint)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the model\n",
    "response = sm.delete_model(ModelName=core_model.model_name)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete roles and policies\n",
    "iam.detach_role_policy(\n",
    "    RoleName=myRoleSageMakerExecution, PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    ")\n",
    "iam.delete_role(RoleName=myRoleSageMakerExecution)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa145c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the parameter store entry\n",
    "ssm.delete_parameter(Name=myParameterStoreChosenModel)\n",
    "ssm.delete_parameter(Name=myParameterStoreEndpointName)\n",
    "ssm.delete_parameter(Name=myParameterStoreIAMARN)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05607d8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Clean Up venv</span>\n",
    "### Clean up if finished with this lab and running in VSCode or equivalent local IDE\n",
    "#### Note these are macOS specific\n",
    "- Run the commands of the cell below in a terminal window if you need to clean up a local venv\n",
    "  - Note if you copy and paste the entire cell and run as one you will get zsh: command not found: # errors because of the comments, but you can ignore\n",
    "  - Remember to restart the kernel to refresh whats available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89229fe8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# if you have local host in your terminal prompt\n",
    "unset HOST\n",
    "# deactivate the venv\n",
    "deactivate \n",
    "# remove it and its contents if not needed\n",
    "rm -rf venv-jumpstart-stable-lab1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jumpstart-stable-lab1 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
