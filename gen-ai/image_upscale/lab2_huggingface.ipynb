{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06920b65",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:DarkSeaGreen\">SageMaker Lab 2</span>\n",
    "\n",
    "This lab does the following:\n",
    "\n",
    "- Provision a HuggingFace model via SageMaker downloading the model\n",
    "- Use a custom container (inference.py provided)\n",
    "- Create a tar file using downloaded model and custom container\n",
    "- Upload tar to S3\n",
    "- Create a SageMaker endpoint\n",
    "- Interacts with the model\n",
    "\n",
    "#### This lab does NOT have any dependencies on Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c39bd",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">requirements_lab2.txt</span>\n",
    "- Most of the requirements just get the latest version\n",
    "- However, on Nov 19 2025 AWS released SageMaker 3.0.1 SDK, this is compatable up to Python 3.12\n",
    "  - At time of writing does not yet support Python 3.13+, (latest current version is Python 3.14)\n",
    "- SageMaker 3.0.1 is forced in the requirements file, otherwise it only gets 3.0.0 which fails due to dependency issues as its sub files are not included, AWS fixed this is 3.0.1 which was released immediately after 3.0.0 :)\n",
    "- Therefore make sure your Python venv is created using 3.12 only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb29b8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Prepare Your Environment</span>\n",
    "### Requirements for this Jupyter Notebook Lab if running in VSCode or equivalent local IDE\n",
    "##### Note these are macOS specific\n",
    "- Credentials\n",
    "  - You need credentials to your AWS account to execute this Jupyter Lab if running locally from your laptop\n",
    "    - Locally: Credentials and therefore permissions asscociated with the IAM user (with CLI access enabled) are provided by AWS configure connection to your AWS account\n",
    "    - Cloud: Permissions provided via logged in user\n",
    "- Installers:\n",
    "  - Pip\n",
    "    - Python libraries\n",
    "    - Works inside Python envs\n",
    "  - homebrew (brew) (mac)\n",
    "    - System software, tools, and dependencies\n",
    "    - Works at OS level\n",
    "\n",
    "- Run the commands of the cell below in a terminal window to create a virtual environment if you need one\n",
    "  - Note check your Python version first, then if ok, copy the rest and run in terminal window\n",
    "  - Note if you copy and paste the multiple lines and run as one you will get zsh: command not found: # errors because of the comments, but you can ignore\n",
    "  - Remember to restart the kernel to pick up the new venv\n",
    "  - The venv can be deleted via the last cell in this notebook iof no longer needed\n",
    "- If you already have a virtual environment, then just activate it as shown in the second cell below\n",
    "  - Venv (can be created below) used by this notebook is *venv-stable-diffuser-lab2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118f0a1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.core import image_uris, script_uris, model_uris\n",
    "model_id, model_version = \"model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16\", \"*\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.4xlarge\",\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(model_id=model_id, model_version=model_version, script_scope=\"inference\")\n",
    "\n",
    "base_model_uri = model_uris.retrieve(model_id=model_id, model_version=model_version, model_scope=\"inference\")\n",
    "\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html\n",
    "print(\"Deploy Image URI (image_uri) - ECR container for the docker image:\\n\", deploy_image_uri)\n",
    "print(\"Deploy Source URI (source_dir) - Location for inference.py:\\n\", deploy_source_uri)\n",
    "print(\"Base Model URI (model_data) - Location of SageMaker model data:\\n\", base_model_uri)   \n",
    "\n",
    "# if you want to see the example inference.py code it uses, you can get it, for example in a terminal window:\n",
    "# aws s3 cp \\\n",
    "#   s3://jumpstart-cache-prod-ap-southeast-2/stabilityai-upscaling/model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16/artifacts/inference-prepack/v1.0.0/code/inference.py \\\n",
    "#   ./inference.py\n",
    "# or see it all\n",
    "# aws s3 ls \\\n",
    "#   s3://jumpstart-cache-prod-ap-southeast-2/stabilityai-upscaling/model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16/ \\\n",
    "#   --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e8873",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Check your credentials (AWS identity) to confirm you are using the right credentials, can also run in a terminal window if you dont have ipykernel (remove the !)\n",
    "!aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27832229",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "### STOP ###\n",
    "### IF USING THIS NOTEBOOK IN A SAGEMAKER JUPYTER NOTEBOOK INSTANCE, THEN SKIP TO THE NEXT CELL ###\n",
    "### OTHERWISE, IF USING VSCODE OR EQUIVALENT LOCAL IDE, THEN CONTINUE BELOW ###\n",
    "### This script is for setting up your environment for the SageMaker Lab 1 ###\n",
    "# do you need to upgrade python first? Your available version of Python is used to create the virtual environment\n",
    "python3 --version\n",
    "\n",
    "### STOP ###\n",
    "### DO YOU NEED TO UPGRADE PYTHON ###\n",
    "# upgrade to the latest version of python if required\n",
    "brew install python==3.12\n",
    "# restart vscode to pickup new version of python\n",
    "python3 --version\n",
    "\n",
    "### STOP ###\n",
    "### OK IF YOU HAVE THE CORRECT VERSION OF PYTHON, CONTINUE ###\n",
    "# create a virtual environment\n",
    "python3.12 -m venv venv-stable-diffuser-lab2\n",
    "# activate the virtual environment\n",
    "source venv-stable-diffuser-lab2/bin/activate\n",
    "### COPY TO HERE ONLY IF RUNNING AS ONE COPY AND PASTE ###\n",
    "\n",
    "### STOP ###\n",
    "### MAKE SURE ABOVE VENV GETS ACTIVATED BEFORE RUNNING THE REST ###\n",
    "# upgrade pip\n",
    "pip install --upgrade pip\n",
    "# jupyter kernel support\n",
    "pip install ipykernel\n",
    "# add the virtual environment to jupyter\n",
    "python  -m ipykernel install --user --name=venv-stable-diffuser-lab2 --display-name \"Python (venv-stable-diffuser-lab2)\"\n",
    "# install the required packages - may need to specify the path here if not in the correct folder in terminal window\n",
    "pip install -r requirements_lab2.txt\n",
    "# pip install -r Documents/github/labs-sagemaker/jumpstart/etc/requirements_lab1.txt\n",
    "# verify the installation\n",
    "pip list\n",
    "\n",
    "### RESTART VSCODE TO PICKUP THE NEW VENV ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcb8f1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "### STOP ###\n",
    "### This command is for activating an environment that already exists, its for use in a terminal window if you need it ###\n",
    "source venv-stable-diffuser-lab2/bin/activate\n",
    "pip list\n",
    "\n",
    "# use pip freeze if you prefer for requirements.txt freiendly format\n",
    "### ALSO MAKE SURE YOU SELECT IT AS YOUR KERNEL FOR THIS JUPYTER NOTEBOOK ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb8cd5",
   "metadata": {},
   "source": [
    "# Lab 1 Starts Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f9b4d",
   "metadata": {},
   "source": [
    "- Vars, libraries and clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86ebb0",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Setup</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# region\n",
    "# for the purpose of this lab, us-east-1, us-west-2, eu-west-1 has the broadest coverage of models and instance types\n",
    "# if you provision in other regions, you may not have access to all the models or instance types, and may need to request increase of quotas for endpoint usage for some instance types\n",
    "myRegion='ap-southeast-2'\n",
    "\n",
    "# iam\n",
    "myRoleSageMakerExecution=\"stable-diffuser-lab2-execution-role\"\n",
    "myRoleSageMakerExecutionARN='RETRIEVED FROM ROLE BELOW'\n",
    "\n",
    "# parameter store\n",
    "myParameterStoreChosenModel='stable-diffuser-lab2-chosen-model'\n",
    "myParameterStoreEndpointName='stable-diffuser-lab2-endpoint-name'\n",
    "myParameterStoreIAMARN='stable-diffuser-lab2-iam-arn'\n",
    "\n",
    "# bucket - MUST BE A UNIQUE NAME\n",
    "myBucket='doit-sagemaker-model-bucket-' + str(random.randint(0, 1000)) + '-' + str(random.randint(0, 1000))\n",
    "\n",
    "# async endpoint\n",
    "myEndpointConfig='stable-diffuser-lab2-endpoint-config'\n",
    "myEndpointAsync='stable-diffuser-lab2-endpoint-async'\n",
    "myEndpoint='stable-diffuser-lab2-endpoint'\n",
    "\n",
    "# model\n",
    "modelID = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c639ad",
   "metadata": {},
   "source": [
    "# <span style=\"color:Coral\">Test Locally - 1!!</span>\n",
    "- <span style=\"color:Coral\">You DO NOT have to run this cell, its for testing the model locally if you want to see it in action first!</span>\n",
    "- This tests the model locally using whatever hardware you have\n",
    "- It downloads the model from huggingface when the cell is run\n",
    "- You probably don't have a GPU, so it will use whatever it can to pipe\n",
    "- This may take a loooong time, may buffer too much memory and fail\n",
    "- We implement tiling to try and compensate, but try with smaller images, eg 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aceef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_upscale import StableDiffusionUpscalePipeline\n",
    "import torch\n",
    "\n",
    "# load model and scheduler\n",
    "pipeline = StableDiffusionUpscalePipeline.from_pretrained(modelID, torch_dtype=torch.float16)\n",
    "\n",
    "# Safe tiling parameters\n",
    "TILE_SIZE = 256          # process 256x256 patches\n",
    "NUM_INFERENCE_STEPS = 25 # reduce to save memory\n",
    "UPSCALE_FACTOR = 4\n",
    "GUIDANCE_SCALE = 7.5\n",
    "\n",
    "\n",
    "# when running this locally, ie NOT on a GPU, you must use a different pipeline device\n",
    "# normally you would pipeline = pipeline.to(\"cuda\") for a GPU\n",
    "# but we are just testing locally here, so we'll use whatever this device supports ... if at all\n",
    "# when we build for AWS later, we will use pipeline = pipeline.to(\"cuda\") in the models container (inference.py)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "pipeline = pipeline.to(device)\n",
    "\n",
    "def tile_image(img, tile_size):\n",
    "    tiles = []\n",
    "    positions = []\n",
    "    sizes = []\n",
    "    width, height = img.size\n",
    "    for y in range(0, height, tile_size):\n",
    "        for x in range(0, width, tile_size):\n",
    "            box = (x, y, min(x + tile_size, width), min(y + tile_size, height))\n",
    "            tile = img.crop(box)\n",
    "            tiles.append(tile)\n",
    "            positions.append((x, y))\n",
    "            sizes.append(tile.size)\n",
    "    return tiles, positions, sizes\n",
    "\n",
    "def stitch_tiles(tiles, positions, sizes, full_size):\n",
    "    final_img = Image.new(\"RGB\", (full_size[0]*UPSCALE_FACTOR, full_size[1]*UPSCALE_FACTOR))\n",
    "    for tile, (x, y), (w, h) in zip(tiles, positions, sizes):\n",
    "        # calculate upscale position and size\n",
    "        up_x, up_y = x*UPSCALE_FACTOR, y*UPSCALE_FACTOR\n",
    "        up_w, up_h = w*UPSCALE_FACTOR, h*UPSCALE_FACTOR\n",
    "        # resize tile to expected upscaled size (in case the model slightly changes it)\n",
    "        tile = tile.resize((up_w, up_h), Image.LANCZOS)\n",
    "        final_img.paste(tile, (up_x, up_y))\n",
    "    return final_img\n",
    "\n",
    "def upscale_with_tiling(img: Image.Image, prompt: str):\n",
    "    \"\"\"Upscale image safely using tiling\"\"\"\n",
    "    tiles, positions, sizes = tile_image(img, TILE_SIZE)\n",
    "    upscaled_tiles = []\n",
    "\n",
    "    for i, tile in enumerate(tiles):\n",
    "        print(f\"Processing tile {i+1}/{len(tiles)}\")\n",
    "        # pipeline expects PIL.Image\n",
    "        tile = tile.convert(\"RGB\")\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            image=tile,\n",
    "            num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "            guidance_scale=GUIDANCE_SCALE\n",
    "        )\n",
    "        # safely get the first image\n",
    "        upscaled_tile = result.images[0]\n",
    "        upscaled_tiles.append(upscaled_tile)\n",
    "\n",
    "    # Combine tiles\n",
    "    return stitch_tiles(upscaled_tiles, positions, sizes, img.size)\n",
    "\n",
    "# use these images\n",
    "prompt = \"Improve the photographic quality of the image\"\n",
    "images = [\n",
    "    \"resources/img2_original_1024.jpeg\",\n",
    "    \"resources/img1_original_1024.jpeg\",\n",
    "]\n",
    "\n",
    "for img in images:\n",
    "    with open(img, \"rb\") as f:\n",
    "        bytes_data = f.read()\n",
    "    pil_image = Image.open(BytesIO(bytes_data)).convert(\"RGB\")\n",
    "    upscaled_image = upscale_with_tiling(pil_image, prompt)\n",
    "    out_path = img.replace(\"original\", \"upscaled\")\n",
    "    upscaled_image.save(out_path)\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808a0e6",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Download Model Locally</span>\n",
    "- We'll download the model so we can upload to S3\n",
    "- When we create the model inference on SageMaker we can then source it from S3 rather than downloading\n",
    "  - Note we don't clean this up locally after the end of the lab, in case you want to run this multiple times\n",
    "  - <span style=\"color:DarkSeaGreen\">We also don't put this into git, so you will only need to execute this cell if you don't already have it</span>\n",
    "    - Look for the model folder, if you don't have it, you need to download the model via the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# download snapshot - we don't require a token for a publicly available model\n",
    "snapshot_dir = snapshot_download(\n",
    "    repo_id=modelID,\n",
    "    local_dir=\"model\",\n",
    "    allow_patterns=[\n",
    "        \"unet/*\",\n",
    "        \"vae/*\",\n",
    "        \"text_encoder/*\",\n",
    "        \"scheduler/*\",\n",
    "        \"tokenizer/*\",\n",
    "        \"feature_extractor/*\",\n",
    "        \"*.json\",    # configs\n",
    "        \"*.txt\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498e46a",
   "metadata": {},
   "source": [
    "# <span style=\"color:Coral\">Test Locally - 2!!</span>\n",
    "- <span style=\"color:Coral\">You DO NOT have to run this cell, its for testing the model locally if you want to see it in action first!</span>\n",
    "- This tests the model locally using whatever hardware you have\n",
    "- It uses the model you just downloaded from huggingface\n",
    "- You probably don't have a GPU, so it will use whatever it can to pipe\n",
    "- This may take a loooong time, may buffer too much memory and fail\n",
    "- Try with smaller images, eg 128x128\n",
    "- This code also exists in testLocal.py in the model_code folder for convenience if you want to test outside of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# use the inference.py from the code folder\n",
    "from model_code.inference import model_fn, input_fn, predict_fn, output_fn\n",
    "\n",
    "# 1. Load the model from your local model directory\n",
    "model_dir = \"./model\"  # path to the folder containing the pretrained pipeline files\n",
    "model = model_fn(model_dir)\n",
    "\n",
    "# 2. Load an image to test\n",
    "image_path = \"resources/img2_original_512.jpeg\"\n",
    "with open(image_path, \"rb\") as f:\n",
    "    img_bytes = f.read()\n",
    "image_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "# 3. Build a dummy payload\n",
    "payload = {\n",
    "    \"prompt\": \"highly detailed, realistic photo\",\n",
    "    \"image\": image_b64\n",
    "}\n",
    "request_body = json.dumps(payload)\n",
    "\n",
    "# 4. Call input_fn\n",
    "inputs = input_fn(request_body, content_type=\"application/json\")\n",
    "\n",
    "# 5. Call predict_fn\n",
    "prediction = predict_fn(inputs, model)\n",
    "\n",
    "# 6. Call output_fn\n",
    "response, content_type = output_fn(prediction, accept=\"application/json\")\n",
    "\n",
    "# 7. Show result\n",
    "response_dict = json.loads(response)\n",
    "upscaled_img_b64 = response_dict[\"image\"]\n",
    "upscaled_img_bytes = base64.b64decode(upscaled_img_b64)\n",
    "upscaled_img = Image.open(BytesIO(upscaled_img_bytes))\n",
    "upscaled_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07c932",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Tar the Model</span>\n",
    "- Create the tar file\n",
    "- The tar file must be of a particular structure, so we need to do the following:\n",
    "  - Copy the code folder (includes inference.py and requirements.txt) into the model folder\n",
    "  - Create the tar, but we dont want the model as a folder, all files in it must be at root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the code folder to the model folder\n",
    "import shutil\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# copy the code file to the downloaded model folder\n",
    "shutil.copytree(\n",
    "    \"model_code\",\n",
    "    \"model/code\",\n",
    "    dirs_exist_ok=True, # overwrite if already exists\n",
    "    ignore=shutil.ignore_patterns(\"__pycache__\", \"__init__.py\", \"testLocal.py\", \"jumpstart_example\"),\n",
    ")\n",
    "\n",
    "# lets make the tar file, its structure is essential\n",
    "tar_name = \"model.tar.gz\"\n",
    "if os.path.exists(tar_name):\n",
    "    os.remove(tar_name)\n",
    "\n",
    "with tarfile.open(tar_name, \"w:gz\") as tar:\n",
    "    for root, dirs, files in os.walk(\"model\"):\n",
    "        # Skip unwanted files/folders\n",
    "        dirs[:] = [d for d in dirs if d not in [\".cache\", \"__pycache__\"]]\n",
    "        files = [f for f in files if f != \".DS_Store\"]\n",
    "\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            # arcname removes the top-level 'model/' folder\n",
    "            arcname = os.path.relpath(full_path, \"model\")\n",
    "            tar.add(full_path, arcname=arcname)\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826415e",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Paths and Clients</span>\n",
    "- setup some paths and clients we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c706918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local client path for resources\n",
    "myLocalPathForResources='/Users/simondavies/Documents/github/labs-sagemaker/gen-ai/image_upscale/'\n",
    "# jupypter notebook path if notebook is used in AWS for example\n",
    "#myLocalPathForResources='/home/ec2-user/SageMaker/labs-sagemaker/gen-ai/image_upscale/'\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import base64\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "from certifi import where\n",
    "from PIL import Image\n",
    "botoSession = boto3.Session(region_name=myRegion)\n",
    "\n",
    "# Configure boto3 to use certifi's certificates - helps avoid SSL errors if your system’s certificate store is out of date or missing root certs\n",
    "sts_client = boto3.client('sts', verify=where())\n",
    "myAccountNumber = sts_client.get_caller_identity()[\"Account\"]\n",
    "print(myAccountNumber)\n",
    "print(sts_client.get_caller_identity()[\"Arn\"])\n",
    "\n",
    "# create clients we can use later\n",
    "# iam\n",
    "iam = boto3.client('iam', region_name=myRegion, verify=where())\n",
    "# ssm\n",
    "ssm = boto3.client('ssm', region_name=myRegion, verify=where())\n",
    "# s3\n",
    "s3 = boto3.client('s3', region_name=myRegion, verify=where())\n",
    "# sagemaker\n",
    "sm = boto3.client(\"sagemaker\", region_name=myRegion, verify=where())\n",
    "# sagemaker runtime\n",
    "smr = boto3.client(\"sagemaker-runtime\", region_name=myRegion, verify=where())\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tags added to all services we create\n",
    "myTags = [\n",
    "    {\"Key\": \"env\", \"Value\": \"non_prod\"},\n",
    "    {\"Key\": \"owner\", \"Value\": \"doit-image-upscale\"},\n",
    "    {\"Key\": \"project\", \"Value\": \"lab1\"},\n",
    "    {\"Key\": \"author\", \"Value\": \"simon\"},\n",
    "]\n",
    "myTagsDct = {\n",
    "    \"env\": \"non_prod\",\n",
    "    \"owner\": \"doit-image-upscale\",\n",
    "    \"project\": \"lab1\",\n",
    "    \"author\": \"simon\",\n",
    "}\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67085b",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">IAM</span>\n",
    "- We need an execution role for SageMaker\n",
    "- So we create one here, only needed if running locally from an IDE, eg VSCode\n",
    "- If in SageKamer Studio Notebook, wont need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSageMakerExecutionRole():\n",
    "    \"\"\"\n",
    "    Creates a role required for SageMaker to run jobs on your behalf\n",
    "    Only needed if this is being run in a local IDE, not needed if in SageMaker Studio or SageMaker Notebook Instance\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        An IAM execution role ARN\n",
    "    \"\"\"\n",
    "\n",
    "    # trust policy for the role\n",
    "    roleTrust = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"sagemaker.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # check if the role exists\n",
    "    try:\n",
    "        role = iam.get_role(RoleName=myRoleSageMakerExecution)\n",
    "        print(\"Role already exists. Using the existing role.\")\n",
    "        return role['Role']['Arn']\n",
    "    except iam.exceptions.NoSuchEntityException:\n",
    "        print(\"Role does not exist. Creating a new role.\")\n",
    "        \n",
    "    # create execution role for sagemaker - allows SageMaker notebook instances, training jobs, and models to access S3, ECR, and CloudWatch on your behalf\n",
    "    # this role is only created if we are running this notebook in a local ide, if we are in a jupyterlab in sagemaker studio, we dont need it as already created and available\n",
    "    role = iam.create_role(\n",
    "        RoleName=myRoleSageMakerExecution,\n",
    "        AssumeRolePolicyDocument=json.dumps(roleTrust),\n",
    "        Description=\"Service excution role for sagemaker ai use including inside jupyter notebooks\",\n",
    "        Tags=[\n",
    "            *myTags,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # attach managed policy to the role AmazonSageMakerFullAccess\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=myRoleSageMakerExecution,\n",
    "        PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n",
    "    )\n",
    "\n",
    "    # store the role arn in parameter store for use in other notebooks\n",
    "    ssm.put_parameter(\n",
    "        Name=myParameterStoreIAMARN,\n",
    "        Description='The ARN of the IAM role used by SageMaker for execution of jobs',\n",
    "        Value=role['Role']['Arn'],\n",
    "        Type='String',\n",
    "        Tags=[\n",
    "            *myTags,\n",
    "        ],\n",
    "    )   \n",
    "\n",
    "    return role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708857ff",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Get Execution Role and Session</span>\n",
    "- SageMaker requires an execution role to assume on your behalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e08fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "sagemaker_session = Session()\n",
    "\n",
    "try:\n",
    "    # if this is being run in a SageMaker AI JupyterLab Notebook\n",
    "    myRoleSageMakerExecutionARN = get_execution_role()\n",
    "except:\n",
    "    # if this is being run in a local IDE - we need to create our own role\n",
    "    myRoleSageMakerExecutionARN = getSageMakerExecutionRole()\n",
    "\n",
    "# make sure we get a session in the correct region (needed as it can use the aws configure region if running this locally\n",
    "sageMakerSession = Session(boto_session=botoSession)\n",
    "\n",
    "print(myRoleSageMakerExecutionARN)\n",
    "print(sageMakerSession)\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56ace1",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Provision a SageMaker Model</span>\n",
    "- Provision a model via a customer inference container\n",
    "  - This container is defined in the inference.py file\n",
    "  - It allows us to download a HuggingFace model directly, and when used customise the use of the GPU via a diffuser pipeline\n",
    "  - We create a custom one because JumpStart models have their own containers and do not allow customisation\n",
    "### Example models to provision\n",
    "- Stable Diffusion x4 upscaler FP16\n",
    "  - https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler/blame/fp16/README.md\n",
    "  - *model_id, model_version = \"model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16\", \"*\"*\n",
    "  - upscaling with Stable Diffusion (x4) is computationally expensive\n",
    "    - FP16 means it uses half-precision floating point, so you want a GPU with good Tensor Core\n",
    "  - the x4 upscaler model itself is large\n",
    "    - want ≥ 16 GB VRAM to run comfortably in FP16 for 512×512 → 2048×2048 upscales\n",
    "    - p4d.24xlarge (enterprise-grade, overkill unless you’re batching lots of requests)\n",
    "      - **needs an aws quota increase for this instance for endpoint usage**\n",
    "    - ml.g5.4xlarge\n",
    "      - good for a poc - widely supported, good memory, reasonably costed\n",
    "      - anything smaller and you will likely get CUDA out of memory errors\n",
    "        - you need plenty of GPU memory\n",
    "      - **needs an aws quota increase for this instance for endpoint usage**\n",
    "- see https://aws.amazon.com/sagemaker/ai/pricing/ for pricing, **larger instances can be very expensive per hour**\n",
    "- If you deply the model and you get a quota error, you will need to visit Service Quotas via the console and request an increase\n",
    "  - go to SageMaker service and search for the instance\n",
    "  - select the *model* for endpoint usage\n",
    "  - make sure your quota allows for auto scaling max\n",
    "- DO NOT LEAVE LARGE INSTANCES RUNNING LONGER THAN YOU NEED TO $$$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768bfb0",
   "metadata": {},
   "source": [
    "### Instance Size is Important\n",
    "- We are usinbg a model that upscales\n",
    "- The larger the original image, the more GPU memory is taken when upscaling\n",
    "- Sagemaker typically uses one GPU to do this \n",
    "  - SageMaker model endpoints don’t automatically spread inference across multiple GPUs unless the container is written for it\n",
    "- Stability Diffusion provides a diffuser library \n",
    "  - Breaks the image into smaller patches, processes sequentially, then stitches\n",
    "  - Uses much less VRAM at the cost of a bit more time\n",
    "  - We use that below\n",
    "- p4d.24xlarge has more GPU memory, but maybe an overkill, expensive and won't scale if source images are still too large to upscale in one GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the instance we want to provision - THIS DISPLAYS AN INPUT BOX FOR YOU TO CHOOSE AN INSTANCE FOR THE MODEL INFERENCE PROVIDED\n",
    "# https://aws.amazon.com/sagemaker/ai/pricing/\n",
    "options = [\n",
    "    f\"img2img|{modelID}|ml.g5.12xlarge $$$$\",\n",
    "    f\"img2img|{modelID}|ml.g4dn.16xlarge $$$$\",\n",
    "    f\"img2img|{modelID}|ml.g5.4xlarge $$$\",\n",
    "    f\"img2img|{modelID}|ml.g5.2xlarge $$\",\n",
    "]\n",
    "\n",
    "print(\"Select an option:\")\n",
    "for i, opt in enumerate(options, 1):\n",
    "    print(f\"{i}. {opt}\")\n",
    "\n",
    "choice = int(input(\"Enter the number of the spec you want: \"))\n",
    "selected = options[choice - 1]\n",
    "\n",
    "modelType = selected.split(\"|\")[0]\n",
    "modelID = selected.split(\"|\")[1]\n",
    "instanceType = selected.split(\"|\")[2].split(\" \")[0]\n",
    "print(f\"You selected: model type {modelType} {modelID} on {instanceType}\")\n",
    "\n",
    "# store the model in a parameter store for use in other labs\n",
    "ssm.put_parameter(\n",
    "    Name=myParameterStoreChosenModel,\n",
    "    Description='the model chosen in lab1',\n",
    "    Value=selected,\n",
    "    Type='String',\n",
    "    Overwrite=True,\n",
    ")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902367f8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Upload inference container and model tar to S3</span>\n",
    "- Create an S3 bucket\n",
    "- Upload the tar model (you will have created this earlier in this lab if not done already)\n",
    "- This can take a long time! 67 minutes if crossing the pacific from Australia to us-east-1 for example!\n",
    "- <span style=\"color:DarkSeaGreen\">You can skip this cell if its already there - dont clean it up at the end of the lab if you want to rerun</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d288f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.s3 import S3Uploader\n",
    "\n",
    "# create a bucket\n",
    "if myRegion == \"us-east-1\":\n",
    "    s3.create_bucket(Bucket=myBucket)\n",
    "else:\n",
    "    s3.create_bucket(\n",
    "        Bucket=myBucket, CreateBucketConfiguration={\"LocationConstraint\": myRegion}\n",
    "    )\n",
    "\n",
    "# Upload each file to the S3 bucket\n",
    "files = [\n",
    "    {\n",
    "        \"s3key\": \"model/model.tar.gz\",\n",
    "        \"localpath\": \"{}model.tar.gz\".format(myLocalPathForResources),\n",
    "    }\n",
    "]\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "for file in files:\n",
    "    print(\"uploading: {}\".format(file[\"localpath\"]))\n",
    "    S3Uploader.upload(local_path=file[\"localpath\"], desired_s3_uri=f\"s3://{myBucket}/model\")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28985330",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Create Model and Endpoint</span>\n",
    "- Create a model from the container\n",
    "- Create a async endpoint config\n",
    "- Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8be4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Elastic Container Registry (ECR) account that hosts official AWS SageMaker PyTorch containers\n",
    "# NOTE\n",
    "# SageMaker spins up a container from this image on your specified instance\n",
    "# The container contains PyTorch + CUDA + Python runtime\n",
    "# Then it loads your model artifact (model.tar.gz) into that container\n",
    "# All inference happens inside that container on the GPU of your instance\n",
    "# When you deploy a model via Model.deploy(), SageMaker pulls this container and runs your model inside it\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-container-images.html?utm_source=chatgpt.com\n",
    "# dkr.ecr - Docker Registry (ECR service)\n",
    "# myRegion (typicaly us-east-1, if not us-east-1 the image has to be changed) - AWS region where the container is stored\n",
    "# amazonaws.com - AWS domain\n",
    "# sagemaker-inference-pytorch - Container name for PyTorch inference\n",
    "# 2.0-gpu-py3 - Tag specifying version: PyTorch 2.0, GPU support, Python 3\n",
    "from sagemaker.core import image_uris\n",
    "try:\n",
    "    aws_ecr_sagemaker_pytorch_container=image_uris.retrieve(framework='neo-pytorch',region=myRegion,version='2.0',image_scope='inference',instance_type=instanceType)\n",
    "except:\n",
    "    print(\"Region not configured for this lab - please use us-east-1 or ap-southeast-2\")\n",
    "    raise Exception(\"Region not configured for this lab - please use us-east-1 or ap-southeast-2\")\n",
    "\n",
    "# If it fails when delpoying the agent, try hardcoding the container string for your region, eg:\n",
    "# 763104351884.dkr.ecr.ap-southeast-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
    "aws_ecr_sagemaker_pytorch_container=\"763104351884.dkr.ecr.ap-southeast-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\"\n",
    "print(f\"Container to use: {aws_ecr_sagemaker_pytorch_container}\")\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dffb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a model object in sagemaker that can be deployed to an endpoint\n",
    "from sagemaker.serve.model_builder import ModelBuilder\n",
    "from sagemaker.serve.utils.types import ModelServer\n",
    "from sagemaker.train.configs import Compute\n",
    "\n",
    "# https://github.com/aws/sagemaker-python-sdk/blob/master/v3-examples/inference-examples/huggingface-example.ipynb\n",
    "compute = Compute(instance_type=instanceType)\n",
    "model_builder = ModelBuilder(\n",
    "    image_uri=aws_ecr_sagemaker_pytorch_container,\n",
    "    s3_model_data_url=f\"s3://{myBucket}/model/model.tar.gz\",\n",
    "    model_server=ModelServer.MMS,\n",
    "    compute=compute,\n",
    "    role_arn=myRoleSageMakerExecutionARN,\n",
    "    sagemaker_session=sageMakerSession,\n",
    ")\n",
    "variantName = \"AllTraffic\"\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d07d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds it\n",
    "core_model = model_builder.build(\n",
    "    model_name=modelID.replace(\"/\", \"-\"),\n",
    "    role_arn=myRoleSageMakerExecutionARN,\n",
    "    sagemaker_session=sageMakerSession,\n",
    ")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dec457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell will create  an endpoint for the model and instance type you selected previously\n",
    "# this will take a while (few minutes), as it needs to get the model from s3, create the endpoint config and then the endpoint\n",
    "# https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler\n",
    "core_endpoint = model_builder.deploy(\n",
    "    endpoint_name=myEndpoint,\n",
    "    role_arn=myRoleSageMakerExecutionARN,\n",
    "    sagemaker_session=sageMakerSession,\n",
    "    container_timeout_in_seconds=600,\n",
    ")\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the endpoint name in a parameter store for use in other notebooks\n",
    "ssm.put_parameter(\n",
    "    Name=myParameterStoreEndpointName,\n",
    "    Description='the name of the sagemaker endpoint created in lab1',\n",
    "    Value=myEndpoint,\n",
    "    Type='String',\n",
    "    Overwrite=True,\n",
    ")\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de692a79",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Sample Images</span>\n",
    "- Create a method to display the images\n",
    "- Upload the sample images to S3, when calling the inference via async, it must use the source from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965df58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required if an image model is being used\n",
    "def decode_and_show(description, model_response) -> None:\n",
    "    from PIL import Image\n",
    "    import base64\n",
    "    import io\n",
    "    \n",
    "    print (description)\n",
    "    # Handle PIL Image objects\n",
    "    if hasattr(model_response, 'save'):  # Check if it's a PIL Image\n",
    "        display(model_response)\n",
    "        return\n",
    "    \n",
    "    # Handle bytes (raw image data)\n",
    "    elif isinstance(model_response, bytes):\n",
    "        image = Image.open(io.BytesIO(model_response))\n",
    "        display(image)\n",
    "        image.close()\n",
    "    \n",
    "    # Handle base64 string (encoded image)\n",
    "    elif isinstance(model_response, str):\n",
    "        image = Image.open(io.BytesIO(base64.b64decode(model_response)))\n",
    "        display(image)\n",
    "        image.close()\n",
    "    \n",
    "    # Handle list of base64 strings (model response)\n",
    "    elif isinstance(model_response, list):\n",
    "        for i, img_data in enumerate(model_response):\n",
    "            image = Image.open(io.BytesIO(base64.b64decode(img_data)))\n",
    "            print(f\"Image {i + 1}:\")\n",
    "            display(image)\n",
    "            image.close()\n",
    "    \n",
    "    else:\n",
    "        print(f\"Can't handle the image. Unexpected response type: {type(model_response)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95541581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload each file to the S3 bucket as a payload for async requests\n",
    "files = [\n",
    "    {\n",
    "        \"s3key\": \"originals/img1_original_1024.json\",\n",
    "        \"localpath\": \"{}/resources/img1_original_1024.jpeg\".format(myLocalPathForResources),\n",
    "        \"prompt\": \"Enhance this image to high-res\",\n",
    "    },\n",
    "    {\n",
    "        \"s3key\": \"originals/img2_original_1024.json\",\n",
    "        \"localpath\": \"{}/resources/img2_original_1024.jpeg\".format(myLocalPathForResources),\n",
    "        \"prompt\": \"Enhance this image to high-res\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Preparing payload for: {file[\"s3key\"]} from {file[\"localpath\"]}\")\n",
    "\n",
    "    # Read and base64 encode the local image\n",
    "    with open(file[\"localpath\"], \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": file[\"prompt\"],\n",
    "        \"image\": image_b64\n",
    "    }\n",
    "    \n",
    "    # Upload JSON payload to S3\n",
    "    s3.put_object(\n",
    "        Bucket=myBucket,\n",
    "        Key=file[\"s3key\"],\n",
    "        Body=json.dumps(payload).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "    print(\"Uploaded payload to s3://{}/{}\".format(myBucket, file[\"s3key\"]))\n",
    "\n",
    "print(\"Done! Move to the next cell ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1209d51",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Invoke the Endpoint</span>\n",
    "- Use the images uploaded from S3 as batch async to the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke, this will return immediately with a job id\n",
    "images = [\n",
    "    \"resources/img1_original_1024.jpeg\",\n",
    "    \"resources/img2_original_1024.jpeg\",\n",
    "]\n",
    "\n",
    "for img in images:\n",
    "    imgOrig = Image.open(img)\n",
    "    # encode image to base64\n",
    "    buffered = io.BytesIO()\n",
    "    imgOrig.save(buffered, format=\"JPEG\")\n",
    "    image_b64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # Resize while preserving aspect ratio\n",
    "    #max_dim = 512\n",
    "    #w, h = imgOrig.size\n",
    "    #if w > h:\n",
    "    #    new_w = max_dim\n",
    "    #    new_h = int(h * max_dim / w)\n",
    "    #else:\n",
    "    #    new_h = max_dim\n",
    "    #    new_w = int(w * max_dim / h)\n",
    "\n",
    "    #imgSmall = imgOrig.resize((new_w, new_h), Image.LANCZOS)\n",
    "    #print (f'Resized the image from (w{w},h{h}) to (w{new_w},h{new_h})')\n",
    "        \n",
    "    # Convert to base64\n",
    "    #buffered = io.BytesIO()\n",
    "    #imgSmall.save(buffered, format=\"PNG\")\n",
    "    #image_b64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": \"highly detailed, realistic photo\",\n",
    "        \"image\": image_b64\n",
    "    }\n",
    "    payload_bytes = json.dumps(payload).encode(\"utf-8\")\n",
    "\n",
    "    # NOTE if you get a 413 error, your original image sizes are probably too large and should be < 6MB\n",
    "    # HTTP 413 Content Too Large (Payload Too Large)\n",
    "    # NOTE if you get a 400 CUDA out of memory error, this indicates that your GPU RAM (Random access memory) is full \n",
    "    # HTTP 400 InternalServerException\n",
    "    # If so, bigger instance with a larger GPU per core, use a diffuser such as below to split the image into batches and restitch once done\n",
    "    print('Starting the inference')\n",
    "\n",
    "    # The following commented code does not work\n",
    "    # Seems SageMaker V3 does not yet support the core_endpoint.invoke method as documented\n",
    "    # https://github.com/aws/sagemaker-python-sdk/blob/master/v3-examples/inference-examples/jumpstart-example.ipynb\n",
    "    # possibly because of a missing __dict__ attribute on the payload object as it has a bytes body\n",
    "    # TypeError: vars() argument must have __dict__ attribute\n",
    "    #response = core_endpoint.invoke(\n",
    "    #    body = payload_bytes,\n",
    "    #    content_type = \"application/json\",\n",
    "    #    accept = \"application/json\",\n",
    "    #)\n",
    "\n",
    "    response = smr.invoke_endpoint(\n",
    "        EndpointName=core_endpoint.endpoint_name,\n",
    "        Body=json.dumps(payload).encode('utf-8'),\n",
    "        ContentType=\"application/json\",\n",
    "        Accept=\"application/json\",\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    #decode_and_show(response[\"generated_image\"])\n",
    "\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e49d4",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Invoke the Endpoint Asynchronously</span>\n",
    "- Use the images uploaded from S3 as batch async to the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke asynchronously, this will return immediately with a job id\n",
    "response = core_endpoint.invoke_async(\n",
    "    input_location=f\"s3://{myBucket}/originals/img1_original.json\",\n",
    "    content_type=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    ")\n",
    "print(f\"OutputLocation: {response[\"OutputLocation\"]}\")\n",
    "print(f\"Submitted async job: {response[\"InferenceId\"]}\")\n",
    "print(f\"Submitted at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfc496",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Move to Lab 2</span>\n",
    "# <span style=\"color:DarkSeaGreen\">OR...</span>\n",
    "# <span style=\"color:DarkSeaGreen\">Clean Up Architecture</span>\n",
    "### <span style=\"color:Red\">Only do this if you have finished with this lab and any labs that depend on it!</span>\n",
    "##### It will delete all architecture created, make sure you no longer need any of it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before you delete, heres the object names in case you want to review them in the console\n",
    "print (f\"Model name: {modelID}\")\n",
    "print(f\"ECR path to container: {aws_ecr_sagemaker_pytorch_container}\")\n",
    "print(f\"S3 uri: s3://{myBucket}/model/model.tar.gz\")\n",
    "print (f\"Iam role ARN: {myRoleSageMakerExecutionARN}\")\n",
    "print(f\"Example inference request: {json.dumps(payload).encode('utf-8')}\")\n",
    "\n",
    "#Tested via console with this:\n",
    "#763104351884.dkr.ecr.ap-southeast-2.amazonaws.com/pytorch-inference:2.1.0-gpu-py310\n",
    "#code currently using this\n",
    "#355873309152.dkr.ecr.ap-southeast-2.amazonaws.com/sagemaker-inference-pytorch:2.0-gpu-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when finished with the endpoint, delete it\n",
    "# if you get an error it may still be updating after scaling in from lab 2 or lab 3 locust tests\n",
    "# try again or delete via the console if the config cannot be found\n",
    "# lets check the endpoint status first to make sure its still not changing due to scaling in\n",
    "response = ssm.get_parameter(\n",
    "    Name=myParameterStoreEndpointName\n",
    ")\n",
    "endpointName = response['Parameter']['Value']\n",
    "response = sm.describe_endpoint(EndpointName=endpointName)\n",
    "print(response[\"EndpointStatus\"])\n",
    "\n",
    "if response[\"EndpointStatus\"] == \"InService\":\n",
    "    print(\"Endpoint is in service. Proceeding with deletion.\")\n",
    "    sm.delete_endpoint(EndpointName=endpointName)\n",
    "    print ('Done! Move to the next cell ->')\n",
    "else:\n",
    "    print(\"Endpoint is not in service. Cannot delete. Try again in a couple of minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the model\n",
    "response = sm.delete_model(ModelName=core_model.model_name)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the endpoint config\n",
    "response = sm.delete_endpoint_config(EndpointConfigName=myEndpoint)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete roles and policies\n",
    "iam.detach_role_policy(\n",
    "    RoleName=myRoleSageMakerExecution, PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    ")\n",
    "iam.delete_role(RoleName=myRoleSageMakerExecution)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa145c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the parameter store entry\n",
    "ssm.delete_parameter(Name=myParameterStoreChosenModel)\n",
    "ssm.delete_parameter(Name=myParameterStoreEndpointName)\n",
    "ssm.delete_parameter(Name=myParameterStoreIAMARN)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae02853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete s3 bucket\n",
    "# NOTE WARNING - this will delete all objects in the bucket with NO prompt or confirmation\n",
    "s3r = boto3.resource('s3', region_name=myRegion, verify=where())\n",
    "bucket = s3r.Bucket(myBucket)\n",
    "bucket.objects.all().delete()\n",
    "\n",
    "# delete the bucket\n",
    "response = s3.delete_bucket(Bucket=myBucket)\n",
    "print ('Done! Move to the next cell ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05607d8",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkSeaGreen\">Clean Up venv</span>\n",
    "### Clean up if finished with this lab and running in VSCode or equivalent local IDE\n",
    "#### Note these are macOS specific\n",
    "- Run the commands of the cell below in a terminal window if you need to clean up a local venv\n",
    "  - Note if you copy and paste the entire cell and run as one you will get zsh: command not found: # errors because of the comments, but you can ignore\n",
    "  - Remember to restart the kernel to refresh whats available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89229fe8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# if you have local host in your terminal prompt\n",
    "unset HOST\n",
    "# deactivate the venv\n",
    "deactivate \n",
    "# remove it and its contents if not needed\n",
    "rm -rf venv-stable-diffuser-lab2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-stable-diffuser-lab2 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
